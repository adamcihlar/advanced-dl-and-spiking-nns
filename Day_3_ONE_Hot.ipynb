{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zmvoww3ds6L6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEfnrv0btWU4"
      },
      "source": [
        "# Colab notebooks remark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd1d5Tf7tV7Y"
      },
      "source": [
        "**Note**: make sure you connect this notebook to a GPU. To do so, please go to main menu and click \"Edit\" -> \"Notebook settings\". In the dialog, select \"GPU\" in the \"Hardware accelerator\" dropdown menu. Don't forget to save the settings. When you see in the upper right corner that your notebook is connected (dropdown menu besides the editing menu) you are ready to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LhBkjZGtVyo"
      },
      "source": [
        "**Origin**: This Colab notebook is based on [this](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-one-hot-encoding-of-words-or-characters.ipynb) Jupyter notebook at GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUjqJoOPs6Lv"
      },
      "source": [
        "# One-hot encoding of words or characters\n",
        "\n",
        "This notebook contains the first code sample found in Chapter 6, Section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). Note that the original text features far more content, in particular further explanations and figures: in this notebook, you will only find source code and related comments.\n",
        "\n",
        "----\n",
        "\n",
        "One-hot encoding is the most common, most basic way to turn a token into a vector. You already saw it in action in our initial IMDB and \n",
        "Reuters examples from chapter 3 (done with words, in our case). It consists in associating a unique integer index to every word, then \n",
        "turning this integer index i into a binary vector of size N, the size of the vocabulary, that would be all-zeros except for the i-th \n",
        "entry, which would be 1.\n",
        "\n",
        "Of course, one-hot encoding can be done at the character level as well. To unambiguously drive home what one-hot encoding is and how to \n",
        "implement it, here are two toy examples of one-hot encoding: one for words, the other for characters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzj38PRNs6Lx"
      },
      "source": [
        "## Word level one-hot encoding (toy example):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "print('Keras version:', keras.__version__)\n",
        "import tensorflow\n",
        "print('Tensorflow version:', tensorflow.__version__)\n",
        "\n",
        "if tensorflow.test.gpu_device_name():\n",
        "  print('Default GPU Device: {}'.format(tensorflow.test.gpu_device_name()))\n",
        "else:\n",
        "  print(\"Please install GPU version of TF\")"
      ],
      "metadata": {
        "id": "QAYhQ4N-U4Of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b4ad101-f93a-4836-915c-ee7eda3f412f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras version: 2.12.0\n",
            "Tensorflow version: 2.12.0\n",
            "Default GPU Device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVRFly25xqyJ"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# This is our initial data; one entry per \"sample\"\n",
        "# (in this toy example, a \"sample\" is just a sentence, but\n",
        "# it could be an entire document).\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# First, build an index of all tokens in the data.\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    \n",
        "    # We simply tokenize the samples via the `split` method.\n",
        "    # in real life, we would also strip punctuation and special characters\n",
        "    # from the samples.\n",
        "    for word in sample.split():\n",
        "        \n",
        "        if word not in token_index:\n",
        "            \n",
        "            # Assign a unique index to each unique word\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            # Note that we don't attribute index 0 to anything."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb__Jf_1xvWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5a951f-e7d9-4507-e024-3e247f70f4fc"
      },
      "source": [
        "token_index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'The': 1,\n",
              " 'cat': 2,\n",
              " 'sat': 3,\n",
              " 'on': 4,\n",
              " 'the': 5,\n",
              " 'mat.': 6,\n",
              " 'dog': 7,\n",
              " 'ate': 8,\n",
              " 'my': 9,\n",
              " 'homework.': 10}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE74eJLRs6Lx"
      },
      "source": [
        "# Next, we vectorize our samples.\n",
        "# We will only consider the first `max_length` words in each sample.\n",
        "max_length = 10\n",
        "\n",
        "# This is where we store our results:\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1."
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-Mp0Fwbx6_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9741eff-d5dd-43b8-f309-74d01b2b00d2"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vHfk_fQyT4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1a9186-09d9-46d5-9d8d-66cb39296666"
      },
      "source": [
        "np.shape(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pf7lnwvs6L0"
      },
      "source": [
        "## Character level one-hot encoding (toy example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Jt2AtNV7LId"
      },
      "source": [
        "import string\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable  # All printable ASCII characters.\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Akt_Nd-7YgU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "77c970ed-0b6c-42cb-f1cf-ef34da8de801"
      },
      "source": [
        "characters"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia0ogKdQ7Qks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded52570-bd72-4c37-b955-bb32947e121d"
      },
      "source": [
        "token_index"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10,\n",
              " 'a': 11,\n",
              " 'b': 12,\n",
              " 'c': 13,\n",
              " 'd': 14,\n",
              " 'e': 15,\n",
              " 'f': 16,\n",
              " 'g': 17,\n",
              " 'h': 18,\n",
              " 'i': 19,\n",
              " 'j': 20,\n",
              " 'k': 21,\n",
              " 'l': 22,\n",
              " 'm': 23,\n",
              " 'n': 24,\n",
              " 'o': 25,\n",
              " 'p': 26,\n",
              " 'q': 27,\n",
              " 'r': 28,\n",
              " 's': 29,\n",
              " 't': 30,\n",
              " 'u': 31,\n",
              " 'v': 32,\n",
              " 'w': 33,\n",
              " 'x': 34,\n",
              " 'y': 35,\n",
              " 'z': 36,\n",
              " 'A': 37,\n",
              " 'B': 38,\n",
              " 'C': 39,\n",
              " 'D': 40,\n",
              " 'E': 41,\n",
              " 'F': 42,\n",
              " 'G': 43,\n",
              " 'H': 44,\n",
              " 'I': 45,\n",
              " 'J': 46,\n",
              " 'K': 47,\n",
              " 'L': 48,\n",
              " 'M': 49,\n",
              " 'N': 50,\n",
              " 'O': 51,\n",
              " 'P': 52,\n",
              " 'Q': 53,\n",
              " 'R': 54,\n",
              " 'S': 55,\n",
              " 'T': 56,\n",
              " 'U': 57,\n",
              " 'V': 58,\n",
              " 'W': 59,\n",
              " 'X': 60,\n",
              " 'Y': 61,\n",
              " 'Z': 62,\n",
              " '!': 63,\n",
              " '\"': 64,\n",
              " '#': 65,\n",
              " '$': 66,\n",
              " '%': 67,\n",
              " '&': 68,\n",
              " \"'\": 69,\n",
              " '(': 70,\n",
              " ')': 71,\n",
              " '*': 72,\n",
              " '+': 73,\n",
              " ',': 74,\n",
              " '-': 75,\n",
              " '.': 76,\n",
              " '/': 77,\n",
              " ':': 78,\n",
              " ';': 79,\n",
              " '<': 80,\n",
              " '=': 81,\n",
              " '>': 82,\n",
              " '?': 83,\n",
              " '@': 84,\n",
              " '[': 85,\n",
              " '\\\\': 86,\n",
              " ']': 87,\n",
              " '^': 88,\n",
              " '_': 89,\n",
              " '`': 90,\n",
              " '{': 91,\n",
              " '|': 92,\n",
              " '}': 93,\n",
              " '~': 94,\n",
              " ' ': 95,\n",
              " '\\t': 96,\n",
              " '\\n': 97,\n",
              " '\\r': 98,\n",
              " '\\x0b': 99,\n",
              " '\\x0c': 100}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zyLmbrzs6L2"
      },
      "source": [
        "# Next, we vectorize our samples.\n",
        "# We will only consider the first `max_length` characters in each sample.\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "\n",
        "# This is where we store our results:\n",
        "for i, sample in enumerate(samples):\n",
        "    \n",
        "    for j, character in enumerate(sample[:max_length]):\n",
        "        index = token_index.get(character)\n",
        "        results[i, j, index] = 1."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIieMeZ27hpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f008badc-3c05-4cba-e9dd-8801e7a6b98b"
      },
      "source": [
        "results"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLIfu8RV7nN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9358d9c-43d9-4e7d-8ceb-412b0dd93251"
      },
      "source": [
        "np.shape(results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 50, 101)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlXeVonzs6L5"
      },
      "source": [
        "Note that Keras has built-in utilities for doing one-hot encoding text at the word level or character level, starting from raw text data. \n",
        "This is what you should actually be using, as it will take care of a number of important features, such as stripping special characters \n",
        "from strings, or only taking into the top N most common words in your dataset (a common restriction to avoid dealing with very large input \n",
        "vector spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmvoww3ds6L6"
      },
      "source": [
        "## Using Keras for word-level one-hot encoding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twyr0ZfqlaqI"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We create a tokenizer, configured to only take\n",
        "# into account the top-1000 most common words\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "sequences = tokenizer.texts_to_sequences(samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANZ8TwBXlfUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8d63e8-7e49-49d2-ad39-2cbf0310dc9e"
      },
      "source": [
        "sequences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4, 1, 5], [1, 6, 7, 8, 9]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVsmw7nex_Bd"
      },
      "source": [
        "What else did the tokenizer learn from the two documents?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcdJW-WmxMPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "043a76ef-b4bc-457c-fc5e-47da2b298479"
      },
      "source": [
        "# How many times a word appears across all documents (samples)?\n",
        "print(tokenizer.word_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('the', 3), ('cat', 1), ('sat', 1), ('on', 1), ('mat', 1), ('dog', 1), ('ate', 1), ('my', 1), ('homework', 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3IAwyZOxhu2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1671138b-1f55-4b2c-8dc1-3b15fe47bfbe"
      },
      "source": [
        "# How many documents (samples) were processed?\n",
        "print(tokenizer.document_count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Yk_EvCxk4m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128d1559-1bfe-461d-ef12-9ba666564308"
      },
      "source": [
        "# Get the token word index\n",
        "print(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbePisf4xxlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73aec460-1fa3-4677-bb8d-8a3ab36025b2"
      },
      "source": [
        "# In how many documents (samples) did a word appear?\n",
        "print(tokenizer.word_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'int'>, {'on': 1, 'the': 2, 'cat': 1, 'mat': 1, 'sat': 1, 'dog': 1, 'ate': 1, 'homework': 1, 'my': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npORIiD3s6L7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35494181-93fa-44d0-9f30-628e8d01bcf5"
      },
      "source": [
        "# You could also directly get the one-hot binary representations.\n",
        "# Note that other vectorization modes than one-hot encoding are supported (see end of Colab notebook)!\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "\n",
        "# This is how you can recover the word index that was computed\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qIRiDzmixy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbfbda7-93e4-4ac9-9b0a-7f6a7532c893"
      },
      "source": [
        "one_hot_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDBcXdY2t5oN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7291e298-a0db-42e2-aba1-a013ee02352b"
      },
      "source": [
        "np.shape(one_hot_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndftb5JHs6MA"
      },
      "source": [
        "\n",
        "A variant of one-hot encoding is the so-called \"one-hot hashing trick\", which can be used when the number of unique tokens in your \n",
        "vocabulary is too large to handle explicitly. Instead of explicitly assigning an index to each word and keeping a reference of these \n",
        "indices in a dictionary, one may hash words into vectors of fixed size. This is typically done with a very lightweight hashing function. \n",
        "The main advantage of this method is that it does away with maintaining an explicit word index, which \n",
        "saves memory and allows online encoding of the data (starting to generate token vectors right away, before having seen all of the available \n",
        "data). The one drawback of this method is that it is susceptible to \"hash collisions\": two different words may end up with the same hash, \n",
        "and subsequently any machine learning model looking at these hashes won't be able to tell the difference between these words. The likelihood \n",
        "of hash collisions decreases when the dimensionality of the hashing space is much larger than the total number of unique tokens being hashed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAsBdI8Fs6MB"
      },
      "source": [
        "## Word-level one-hot encoding with hashing trick (toy example):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hashing trick is useful when the model is learning on-line, i.e. when we cannot acquire a token index before modelling. Another advantage is that we save memory, since we don't have token index and we don't need to store it. However, if dimensionality is set too low, hash collisions may happen."
      ],
      "metadata": {
        "id": "5CAqIf23t10y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgepfQFus6MB"
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We will store our words as vectors of size 1000.\n",
        "# Note that if you have close to 1000 words (or more)\n",
        "# you will start seeing many hash collisions, which\n",
        "# will decrease the accuracy of this encoding method.\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "\n",
        "for i, sample in enumerate(samples):\n",
        "\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # Hash the word into a \"random\" integer index\n",
        "        # that is between 0 and 1000\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        results[i, j, index] = 1."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGRrZRIfu9W1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a897d59-d318-4885-cb18-972a05b9e6a7"
      },
      "source": [
        "results"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RkaVgneu-6V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1132e258-c0dd-4820-856c-c9b33f8c3593"
      },
      "source": [
        "np.shape(results)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j58dhgzfvBcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb0b2f8-7d97-4091-84f9-6cc06d413ccf"
      },
      "source": [
        "for i, sample in enumerate(samples):\n",
        "\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        # Hash the word into a \"random\" integer index\n",
        "        # that is between 0 and 1000\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        print(word, '\\t\\t',  hash(word), '\\t', abs(hash(word)), '\\t', abs(hash(word)) % dimensionality)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The \t\t -2816866875701909779 \t 2816866875701909779 \t 779\n",
            "cat \t\t 8719935608692621737 \t 8719935608692621737 \t 737\n",
            "sat \t\t -7813532714720098285 \t 7813532714720098285 \t 285\n",
            "on \t\t 8183384042238474669 \t 8183384042238474669 \t 669\n",
            "the \t\t 5095630944560830428 \t 5095630944560830428 \t 428\n",
            "mat. \t\t 57538674282556307 \t 57538674282556307 \t 307\n",
            "The \t\t -2816866875701909779 \t 2816866875701909779 \t 779\n",
            "dog \t\t -7000368803002212508 \t 7000368803002212508 \t 508\n",
            "ate \t\t 7071214967748246649 \t 7071214967748246649 \t 649\n",
            "my \t\t -1868773100422285156 \t 1868773100422285156 \t 156\n",
            "homework. \t\t 7369738959097316453 \t 7369738959097316453 \t 453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZ_0WI7Noo7M"
      },
      "source": [
        "**NOTE**: **%** is the modulus operator. It divides left hand operand by right hand operand and returns remainder. That means if we set `dimensionality` to 1000, we will yield integers between 0 and 999. These integers will then be used as indices for the `results` tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az45aWxQrI6l"
      },
      "source": [
        "## Keras built-in methods for text processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtzMnkwFsBwz"
      },
      "source": [
        "There are serval methods in the Keras Text module (`keras.preprocessing.text`) that automate the processing of text sequences. The links below will direct you to the Keras documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3veHofmArVci"
      },
      "source": [
        "Keras [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r_F2yLxrjZL"
      },
      "source": [
        "Keras [`hashing_trick`](https://keras.io/preprocessing/text/#hashing_trick)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDxoNh7ZsQ2j"
      },
      "source": [
        "Keras [`one_hot`](https://keras.io/preprocessing/text/#one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1t1xFdTsbWk"
      },
      "source": [
        "Keras [`text_to_word_sequence`](https://keras.io/preprocessing/text/#text_to_word_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbIQ7b04ti5U"
      },
      "source": [
        "Keras [`texts_to_matrix`](https://keras.rstudio.com/reference/texts_to_matrix.html) - **NOTE**: this is the documentation of Keras for **R**.\n",
        "* `binary`: Whether or not each word is present in the document. This is the default.\n",
        "* `count`: The count of each word in the document.\n",
        "* `tfidf`: The Term Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "* `freq`: The frequency of each word as a ratio of words within each document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n179bH05tihl"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "# We create a tokenizer, configured to only take\n",
        "# into account the top-1000 most common words\n",
        "tokenizer = Tokenizer(num_words=10)\n",
        "\n",
        "# This builds the word index\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "# This turns strings into lists of integer indices.\n",
        "mode_binary = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "mode_count = tokenizer.texts_to_matrix(samples, mode='count')\n",
        "mode_tfidf = tokenizer.texts_to_matrix(samples, mode='tfidf')\n",
        "mode_freq = tokenizer.texts_to_matrix(samples, mode='freq')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAbSYBOmuWo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803d66cd-0306-4fdb-93bd-511310f5db2a"
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 1, 'cat': 2, 'sat': 3, 'on': 4, 'mat': 5, 'dog': 6, 'ate': 7, 'my': 8, 'homework': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBS2tDDDttgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5209f424-beda-4d3d-9922-010915071f47"
      },
      "source": [
        "mode_binary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFLsWYALvRcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0bd8704-8efd-44cc-c5fe-d019ae132643"
      },
      "source": [
        "np.shape(mode_binary)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-vqCPn5uOcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d84e98-3b02-4071-efe3-5d62dc423eac"
      },
      "source": [
        "mode_count"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 2., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFWjDseavVCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a6f1b8-d717-4875-d9e0-0eb771adda50"
      },
      "source": [
        "np.shape(mode_count)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jjPRk2Fu23N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d095964-8925-4bbe-dcd1-7e94f36d9716"
      },
      "source": [
        "mode_tfidf"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.86490296, 0.69314718, 0.69314718, 0.69314718,\n",
              "        0.69314718, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.51082562, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.69314718, 0.69314718, 0.69314718, 0.69314718]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZHyPUBqvbHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a59d71b9-90dd-4162-eed8-5aba1cd6180a"
      },
      "source": [
        "np.shape(mode_tfidf)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz2KL5ayvDqs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f59c077-e804-4a33-be3e-e7ce1c9fb77e"
      },
      "source": [
        "mode_freq"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.33333333, 0.16666667, 0.16666667, 0.16666667,\n",
              "        0.16666667, 0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.2       , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.2       , 0.2       , 0.2       , 0.2       ]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqVp0-jjvfgk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d55f771-eb17-4aa8-eaad-ba2af88772c0"
      },
      "source": [
        "np.shape(mode_freq)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4TFuHtxvk00"
      },
      "source": [
        "Find out which methods and attributes are available in class Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTjN7VPOo_-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3adc03-ea92-4752-e840-3a1685f38c45"
      },
      "source": [
        "dir(tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__class__',\n",
              " '__delattr__',\n",
              " '__dict__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__lt__',\n",
              " '__module__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__weakref__',\n",
              " '_keras_api_names',\n",
              " '_keras_api_names_v1',\n",
              " 'analyzer',\n",
              " 'char_level',\n",
              " 'document_count',\n",
              " 'filters',\n",
              " 'fit_on_sequences',\n",
              " 'fit_on_texts',\n",
              " 'get_config',\n",
              " 'index_docs',\n",
              " 'index_word',\n",
              " 'lower',\n",
              " 'num_words',\n",
              " 'oov_token',\n",
              " 'sequences_to_matrix',\n",
              " 'sequences_to_texts',\n",
              " 'sequences_to_texts_generator',\n",
              " 'split',\n",
              " 'texts_to_matrix',\n",
              " 'texts_to_sequences',\n",
              " 'texts_to_sequences_generator',\n",
              " 'to_json',\n",
              " 'word_counts',\n",
              " 'word_docs',\n",
              " 'word_index']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4PH9CfyuzOE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90aad8d7-97ab-4d1d-dc4e-242ed1f6fecb"
      },
      "source": [
        "hash('Oliver')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8368648330736616601"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzFGgfKdY4y3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e79abe61-13fe-4e2f-f863-beabab5561cf"
      },
      "source": [
        "abs(hash('Oliver'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8368648330736616601"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmqk8yYRY8-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44066d6-88cf-4bbf-a290-bcbb5de3d667"
      },
      "source": [
        "abs(hash('Oliver')) % 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdnZd8FwZEZW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89d515af-0807-4ac5-daf7-9c8cac1e16d8"
      },
      "source": [
        "abs(hash('Stavanger')) % 10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    }
  ]
}