{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8wRI0pP_54y",
        "outputId": "77f0e942-5822-41d7-8eb9-aab39b7f8d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# connect the google drive\n",
        "drive.mount('/content/drive')\n",
        "# paths, parameters\n",
        "PATH = 'drive/MyDrive/Colab_Notebooks_2/dlsnn_project'\n",
        "PATH_DATA = os.path.join(PATH, 'data')\n",
        "PATH_MODELS = os.path.join(PATH, 'models')\n",
        "PATH_ASSETS = os.path.join(PATH, 'assets')\n",
        "\n",
        "np.random.seed(seed=42)\n",
        "\n",
        "# create the subdirectories\n",
        "!mkdir -p $PATH_DATA\n",
        "!mkdir -p $PATH_MODELS\n",
        "!mkdir -p $PATH_ASSETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "293s1WDPACC2"
      },
      "outputs": [],
      "source": [
        "# unzip and move the datasets\n",
        "if False:\n",
        "  !unzip $PATH_DATA'/AWA2.zip'\n",
        "  !mv AWA2 $PATH_DATA\n",
        "\n",
        "  !unzip $PATH_DATA'/CUB.zip'\n",
        "  !mv CUB $PATH_DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG_CmZj1uStK"
      },
      "outputs": [],
      "source": [
        "# install all the needed libraries from requirements file\n",
        "!pip install -r -q $PATH'/requirements.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UPx52eXFAE3_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import autograd\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import io, spatial\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
        "from sklearn.decomposition import PCA\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as K\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mWNVTwYpAIVL"
      },
      "outputs": [],
      "source": [
        "def load_data(dataset='AWA2', bin_att=True):\n",
        "    '''\n",
        "    dataset:\n",
        "      'AWA2' or 'CUB' or any other ZSL dataset following the same structure and saved in PATH_DATA\n",
        "    '''\n",
        "    res101 = io.loadmat(os.path.join(PATH_DATA, dataset, 'res101.mat'))\n",
        "    att_splits=io.loadmat(os.path.join(PATH_DATA, dataset, 'att_splits.mat'))\n",
        "    if dataset=='AWA2':\n",
        "        bin_att_splits=io.loadmat(os.path.join(PATH_DATA, dataset, 'binaryAtt_splits.mat'))\n",
        "\n",
        "    # train, val, test predefined splits\n",
        "    train_loc = 'train_loc'\n",
        "    val_loc = 'val_loc'\n",
        "    test_loc = 'test_unseen_loc'\n",
        "\n",
        "    cl_trainval_loc = 'trainval_loc'\n",
        "    cl_test_loc = 'test_seen_loc'\n",
        "\n",
        "    # train and val splits\n",
        "    train_loc = att_splits[train_loc]\n",
        "    val_loc = att_splits[val_loc]\n",
        "\n",
        "    # split the features\n",
        "    feat = res101['features']\n",
        "    # matlab indexing starts from 1 -> -1\n",
        "    # one type of splits based on set of seen/unseen labels\n",
        "    X_train = feat[:, np.squeeze(train_loc - 1)].T\n",
        "    X_val = feat[:, np.squeeze(val_loc - 1)].T\n",
        "    X_test_u = feat[:, np.squeeze(att_splits[test_loc] - 1)].T\n",
        "    # second type of split - splits concatenation of X_train and X_val\n",
        "    # but all classes are represented in both subsets\n",
        "    # for classifier development\n",
        "    X_trainval_s = feat[:, np.squeeze(att_splits[cl_trainval_loc]-1)].T\n",
        "    X_test_s = feat[:, np.squeeze(att_splits[cl_test_loc]-1)].T\n",
        "\n",
        "    # split the labels\n",
        "    labels = res101['labels']\n",
        "    # matlab indexing starts from 1 -> -1\n",
        "    y_train = np.squeeze(labels[np.squeeze(train_loc - 1)])\n",
        "    y_val = np.squeeze(labels[np.squeeze(val_loc - 1)])\n",
        "    y_test_u = np.squeeze(labels[np.squeeze(att_splits[test_loc]-1)])\n",
        "\n",
        "    y_trainval_s = np.squeeze(labels[np.squeeze(att_splits[cl_trainval_loc]-1)])\n",
        "    y_test_s = np.squeeze(labels[np.squeeze(att_splits[cl_test_loc]-1)])\n",
        "\n",
        "    # also the labels start with 1\n",
        "    y_train -= 1\n",
        "    y_val -= 1\n",
        "    y_test_u -= 1\n",
        "    y_trainval_s -= 1\n",
        "    y_test_s -= 1\n",
        "\n",
        "    # divide the labels to seen and unseen\n",
        "    y_train_seen = np.unique(y_train)\n",
        "    y_val_unseen = np.unique(y_val)\n",
        "    y_test_unseen = np.unique(y_test_u)\n",
        "\n",
        "    # divide the attributes to seen and unseen\n",
        "    if dataset=='AWA2' and bin_att:\n",
        "        att = bin_att_splits['att']\n",
        "    elif dataset=='CUB' and bin_att:\n",
        "        att = att_splits['original_att']\n",
        "        att[att!=0] = 1\n",
        "    else:\n",
        "        att = att_splits['att']\n",
        "    att_train_seen = att[:, y_train_seen]\n",
        "    att_val_unseen = att[:, y_val_unseen]\n",
        "    att_test_unseen = att[:, y_test_unseen]\n",
        "\n",
        "    # create numpy arrays with attributes for all splits\n",
        "    # assign corresponding attribute vector to a given label\n",
        "    att_train = np.empty((y_train.shape[0], att.shape[0]))\n",
        "    for i in range(y_train.shape[0]):\n",
        "        att_train[i] = att[:, y_train[i]]\n",
        "    att_val = np.empty((y_val.shape[0], att.shape[0]))\n",
        "    for i in range(y_val.shape[0]):\n",
        "        att_val[i] = att[:, y_val[i]]\n",
        "    att_test_u = np.empty((y_test_u.shape[0], att.shape[0]))\n",
        "    for i in range(y_test_u.shape[0]):\n",
        "        att_test_u[i] = att[:, y_test_u[i]]\n",
        "    att_trainval_s = np.empty((y_trainval_s.shape[0], att.shape[0]))\n",
        "    for i in range(y_trainval_s.shape[0]):\n",
        "        att_trainval_s[i] = att[:, y_trainval_s[i]]\n",
        "    att_test_s = np.empty((y_test_s.shape[0], att.shape[0]))\n",
        "    for i in range(y_test_s.shape[0]):\n",
        "        att_test_s[i] = att[:, y_test_s[i]]\n",
        "\n",
        "    # save all to dictionaries\n",
        "    train = {'X': X_train, 'y': y_train, 'att': att_train}\n",
        "    val = {'X': X_val, 'y': y_val, 'att': att_val}\n",
        "    test_u = {'X': X_test_u, 'y': y_test_u, 'att': att_test_u}\n",
        "\n",
        "    trainval_s = {'X': X_trainval_s, 'y': y_trainval_s, 'att': att_trainval_s}\n",
        "    test_s = {'X': X_test_s, 'y': y_test_s, 'att': att_test_s}\n",
        "\n",
        "    # get unique labels and attributes\n",
        "    knn_att = att.T\n",
        "    knn_y = np.arange(len(knn_att))\n",
        "\n",
        "    # get the class names for text embedding\n",
        "    class_names = att_splits['allclasses_names']\n",
        "\n",
        "    return train, val, test_u, trainval_s, test_s, knn_att, knn_y, y_test_unseen, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C8R8b2ljYxL"
      },
      "source": [
        "## AWA2 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mnjX_JGGnAm"
      },
      "outputs": [],
      "source": [
        "# get all the splits\n",
        "train, val, test_u, trainval_s, test_s, knn_att, knn_y, y_test_unseen, class_names = load_data('AWA2', True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vLhrNIINqsX"
      },
      "outputs": [],
      "source": [
        "# split the trainval dataset\n",
        "train_X, val_X, train_att, val_att, train_y, val_y = train_test_split(trainval_s['X'], trainval_s['att'], trainval_s['y'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjNQWA8Mdf2z"
      },
      "source": [
        "### DAP\n",
        "One classifier per attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxZy9hmnEwKY"
      },
      "outputs": [],
      "source": [
        "# define a common architecture of a model for attribute prediction\n",
        "def get_attribute_mlp(input_size=512, hidden_size=256, n_attributes=1, lr=0.001):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(input_size, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(hidden_size, activation='relu'))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(n_attributes, activation='sigmoid'))\n",
        "\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model.compile(optimizer=opt,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sP6OuPqYeOWu",
        "outputId": "d9e61ab7-cfdb-4d19-cac0-e22d851027e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preserved ratio of variance after applying PCA:  0.9120702050900669\n"
          ]
        }
      ],
      "source": [
        "# reduce the dimensionality of the features to reduce the size of the models\n",
        "pca = PCA(n_components=512)\n",
        "train_X_pca = pca.fit_transform(train_X)\n",
        "print('Preserved ratio of variance after applying PCA: ', sum(pca.explained_variance_ratio_))\n",
        "val_X_pca = pca.transform(val_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjAEgYYneob8"
      },
      "outputs": [],
      "source": [
        "# for every attribute train a separate model\n",
        "for attribute in tqdm(range(train_att.shape[1])):\n",
        "    train_y = train_att[:, attribute]\n",
        "    val_y = val_att[:, attribute]\n",
        "\n",
        "    K.clear_session()\n",
        "    model = get_attribute_mlp(input_size=512, n_attributes=1, lr=0.001)\n",
        "    history = model.fit(\n",
        "        train_X_pca,\n",
        "        train_y,\n",
        "        validation_data=(val_X_pca, val_y),\n",
        "        epochs=2,\n",
        "        verbose=0,\n",
        "    )\n",
        "    model.save_weights(os.path.join(PATH_MODELS, 'mlp_bin_att_' + str(attribute)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpGyhykqvxb9"
      },
      "outputs": [],
      "source": [
        "# transform the test sets using fitted PCA\n",
        "test_u_X_pca = pca.transform(test_u['X'])\n",
        "test_s_X_pca = pca.transform(test_s['X'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QqLRlqVALvis"
      },
      "outputs": [],
      "source": [
        "# get attibute predictions for test sets (seen and unseen)\n",
        "# prepare empty arrays to store the predicted attributes\n",
        "test_u_att_pred = np.zeros((test_u_X_pca.shape[0], knn_att.shape[1]))\n",
        "test_s_att_pred = np.zeros((test_s_X_pca.shape[0], knn_att.shape[1]))\n",
        "\n",
        "# for every attribute get the predictions from corresponding model\n",
        "for attribute in tqdm(range(knn_att.shape[1])):\n",
        "    K.clear_session()\n",
        "    model = get_attribute_mlp(input_size=512, n_attributes=1, lr=0.001)\n",
        "    model.load_weights(os.path.join(PATH_MODELS, 'mlp_bin_att_' + str(attribute)))\n",
        "\n",
        "    test_u_att_pred[:, attribute] = model.predict(test_u_X_pca, verbose=0).flatten()\n",
        "    test_s_att_pred[:, attribute] = model.predict(test_s_X_pca, verbose=0).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WH8IhoOKLPqG",
        "outputId": "8eeb2eed-f25a-4f8d-8a68-ded1d78e979b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           6      0.186     0.214     0.199      1645\n",
            "           8      0.147     0.989     0.256       174\n",
            "          22      0.416     0.704     0.523      1420\n",
            "          23      0.389     0.222     0.282       988\n",
            "          29      1.000     0.005     0.010       383\n",
            "          30      0.319     0.048     0.084      1202\n",
            "          33      0.437     0.932     0.595       310\n",
            "          40      0.832     0.889     0.860       630\n",
            "          46      0.286     0.047     0.080       215\n",
            "          49      0.348     0.122     0.180       946\n",
            "\n",
            "    accuracy                          0.351      7913\n",
            "   macro avg      0.436     0.417     0.307      7913\n",
            "weighted avg      0.395     0.351     0.305      7913\n",
            "\n",
            "[[ 352    0 1218    0    0   71    1    3    0    0]\n",
            " [   0  172    0    2    0    0    0    0    0    0]\n",
            " [ 326    0 1000   10    0   39   31   14    0    0]\n",
            " [   9  390   40  219    0    3  105   17   24  181]\n",
            " [  97    5   93    1    2    1  172    8    0    4]\n",
            " [1078    0    0    0    0   58    0   66    0    0]\n",
            " [   4    0   15    1    0    0  289    1    0    0]\n",
            " [  11    0    3    0    0    1   55  560    0    0]\n",
            " [   9   85   33   27    0    8    9    4   10   30]\n",
            " [   5  520    1  303    0    1    0    0    1  115]]\n"
          ]
        }
      ],
      "source": [
        "# find the nearest neighbor based on the predicted attributes found in the image to infer the class\n",
        "# unseen classes\n",
        "knn = KNN(n_neighbors=1)\n",
        "knn.fit(knn_att[y_test_unseen], y_test_unseen)\n",
        "test_preds = knn.predict(test_u_att_pred)\n",
        "print(classification_report(test_u['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_u['y'], test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QhsQGbG0GlG",
        "outputId": "623475c8-a0dd-4575-9302-7008d3314458"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.875     0.923     0.898       182\n",
            "           1      0.962     0.932     0.947       162\n",
            "           2      0.981     0.883     0.930        60\n",
            "           3      0.875     0.683     0.767        41\n",
            "           4      0.981     0.981     0.981       107\n",
            "           5      0.944     0.912     0.928       148\n",
            "           7      0.902     0.885     0.894       218\n",
            "           9      0.873     0.890     0.881       100\n",
            "          10      1.000     0.968     0.984        31\n",
            "          11      0.818     0.720     0.766        25\n",
            "          12      1.000     0.994     0.997       175\n",
            "          13      0.901     0.965     0.932       141\n",
            "          14      1.000     1.000     1.000       152\n",
            "          15      0.907     0.847     0.876       150\n",
            "          16      0.809     0.704     0.752        54\n",
            "          17      0.947     0.986     0.966       144\n",
            "          18      0.979     0.984     0.982       189\n",
            "          19      0.964     0.975     0.970       163\n",
            "          20      0.886     0.836     0.860       140\n",
            "          21      0.984     0.984     0.984       124\n",
            "          24      0.909     0.920     0.914       162\n",
            "          25      0.894     0.971     0.931       139\n",
            "          26      0.955     0.987     0.971       239\n",
            "          27      0.977     0.956     0.966       135\n",
            "          28      0.991     1.000     0.995       211\n",
            "          31      0.957     0.924     0.940       119\n",
            "          32      0.877     0.840     0.858       119\n",
            "          34      0.765     0.780     0.772        50\n",
            "          35      0.927     0.922     0.924       166\n",
            "          36      0.926     0.959     0.943       197\n",
            "          37      1.000     1.000     1.000       228\n",
            "          38      1.000     0.995     0.997       182\n",
            "          39      0.921     0.935     0.928       275\n",
            "          41      0.965     0.897     0.930       155\n",
            "          42      0.995     0.980     0.987       198\n",
            "          43      0.759     0.550     0.638        40\n",
            "          44      0.995     0.989     0.992       184\n",
            "          45      0.889     0.954     0.921       219\n",
            "          47      0.939     0.894     0.916       104\n",
            "          48      0.873     0.945     0.907       254\n",
            "\n",
            "    accuracy                          0.938      5882\n",
            "   macro avg      0.928     0.911     0.918      5882\n",
            "weighted avg      0.938     0.938     0.938      5882\n",
            "\n",
            "[[168   1   0 ...   0   0   0]\n",
            " [  1 151   0 ...   0   0   0]\n",
            " [  0   0  53 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 209   0   2]\n",
            " [  0   0   0 ...   0  93   0]\n",
            " [  1   0   0 ...   1   0 240]]\n"
          ]
        }
      ],
      "source": [
        "# find the nearest neighbor based on the predicted attributes found in the image to infer the class\n",
        "# seen classes\n",
        "knn = KNN(n_neighbors=1)\n",
        "msk = [x not in y_test_unseen for x in knn_y]\n",
        "knn.fit(knn_att[msk], knn_y[msk])\n",
        "# seen test classes\n",
        "test_preds = knn.predict(test_s_att_pred)\n",
        "print(classification_report(test_s['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_s['y'], test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af8t7l-24jQh",
        "outputId": "06ee74e4-a8a2-425a-9a5a-f562e278ce74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           6      0.174     0.193     0.183      1645\n",
            "           8      0.140     0.983     0.245       174\n",
            "          22      0.389     0.663     0.490      1420\n",
            "          23      0.379     0.232     0.288       988\n",
            "          29      0.500     0.008     0.015       383\n",
            "          30      0.411     0.082     0.137      1202\n",
            "          33      0.381     0.868     0.530       310\n",
            "          40      0.802     0.786     0.794       630\n",
            "          46      0.226     0.033     0.057       215\n",
            "          49      0.355     0.088     0.141       946\n",
            "\n",
            "    accuracy                          0.331      7913\n",
            "   macro avg      0.376     0.394     0.288      7913\n",
            "weighted avg      0.370     0.331     0.291      7913\n",
            "\n",
            "[[ 318    1 1248    0    0   70    4    3    0    1]\n",
            " [   0  171    0    2    0    0    0    0    0    1]\n",
            " [ 355    1  942   14    1   64   33   10    0    0]\n",
            " [  10  427   53  229    0    1  114   14   22  118]\n",
            " [  87    5  104    4    3    1  164    9    1    5]\n",
            " [1021    0    2    0    0   99    0   80    0    0]\n",
            " [  11    0   28    1    0    0  269    1    0    0]\n",
            " [  14    0    8    0    2    0  111  495    0    0]\n",
            " [   7   93   36   26    0    5   11    4    7   26]\n",
            " [   6  525    1  328    0    1    0    1    1   83]]\n"
          ]
        }
      ],
      "source": [
        "# try with thresholded values -> Hamming Distance\n",
        "knn = KNN(n_neighbors=1)\n",
        "knn.fit(knn_att[y_test_unseen], y_test_unseen)\n",
        "test_preds = knn.predict(test_u_att_pred.round())\n",
        "print(classification_report(test_u['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_u['y'], test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7ZIjHjEdGVq",
        "outputId": "7978a699-3e94-4fd7-871f-7e450ac0bc58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.850     0.934     0.890       182\n",
            "           1      0.950     0.938     0.944       162\n",
            "           2      0.982     0.917     0.948        60\n",
            "           3      0.853     0.707     0.773        41\n",
            "           4      0.972     0.981     0.977       107\n",
            "           5      0.925     0.912     0.918       148\n",
            "           7      0.911     0.890     0.900       218\n",
            "           9      0.864     0.890     0.877       100\n",
            "          10      1.000     0.968     0.984        31\n",
            "          11      0.833     0.800     0.816        25\n",
            "          12      1.000     0.994     0.997       175\n",
            "          13      0.912     0.957     0.934       141\n",
            "          14      0.993     1.000     0.997       152\n",
            "          15      0.906     0.840     0.872       150\n",
            "          16      0.816     0.741     0.777        54\n",
            "          17      0.959     0.986     0.973       144\n",
            "          18      0.974     0.984     0.979       189\n",
            "          19      0.963     0.969     0.966       163\n",
            "          20      0.887     0.843     0.864       140\n",
            "          21      0.976     0.984     0.980       124\n",
            "          24      0.909     0.926     0.917       162\n",
            "          25      0.893     0.957     0.924       139\n",
            "          26      0.948     0.987     0.967       239\n",
            "          27      0.956     0.963     0.959       135\n",
            "          28      0.991     1.000     0.995       211\n",
            "          31      0.948     0.924     0.936       119\n",
            "          32      0.903     0.857     0.879       119\n",
            "          34      0.755     0.800     0.777        50\n",
            "          35      0.944     0.910     0.926       166\n",
            "          36      0.931     0.959     0.945       197\n",
            "          37      1.000     1.000     1.000       228\n",
            "          38      1.000     0.995     0.997       182\n",
            "          39      0.927     0.927     0.927       275\n",
            "          41      0.965     0.890     0.926       155\n",
            "          42      1.000     0.985     0.992       198\n",
            "          43      0.808     0.525     0.636        40\n",
            "          44      0.995     0.989     0.992       184\n",
            "          45      0.896     0.945     0.920       219\n",
            "          47      0.939     0.885     0.911       104\n",
            "          48      0.880     0.925     0.902       254\n",
            "\n",
            "    accuracy                          0.938      5882\n",
            "   macro avg      0.928     0.915     0.920      5882\n",
            "weighted avg      0.938     0.938     0.938      5882\n",
            "\n",
            "[[170   1   0 ...   0   0   0]\n",
            " [  1 152   0 ...   0   1   0]\n",
            " [  0   0  55 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 207   0   2]\n",
            " [  0   0   0 ...   0  92   0]\n",
            " [  1   0   0 ...   3   0 235]]\n"
          ]
        }
      ],
      "source": [
        "# try with thresholded values -> Hamming Distance\n",
        "knn = KNN(n_neighbors=1)\n",
        "msk = [x not in y_test_unseen for x in knn_y]\n",
        "knn.fit(knn_att[msk], knn_y[msk])\n",
        "test_preds = knn.predict(test_s_att_pred.round())\n",
        "print(classification_report(test_s['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_s['y'], test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjFkgQpidpoQ"
      },
      "source": [
        "### Joint model for attribute prediction\n",
        "Having one model for each attribute is a little bit heavy, try training joint model for all attributes at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "KBhkSQAWXsX0"
      },
      "outputs": [],
      "source": [
        "# get all the splits\n",
        "train, val, test_u, trainval_s, test_s, knn_att, knn_y, y_test_unseen, class_names = load_data('AWA2', bin_att=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "uVI2w4qDcdCe"
      },
      "outputs": [],
      "source": [
        "# split the trainval dataset\n",
        "train_X, val_X, train_att, val_att, train_y, val_y = train_test_split(trainval_s['X'], trainval_s['att'], trainval_s['y'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "257H8iLhdut2"
      },
      "outputs": [],
      "source": [
        "# train one big model for all attributes\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(85, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt,\n",
        "          loss='binary_crossentropy',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_X,\n",
        "    train_att,\n",
        "    validation_data=(val_X, val_att),\n",
        "    epochs=20,\n",
        "    verbose=1,\n",
        "    callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(PATH_MODELS, 'awa_mlp_att_full_val'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msCxKspxqeev"
      },
      "outputs": [],
      "source": [
        "best_epochs = np.argmin(pd.DataFrame(history.history).val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITWKQbCoqPth",
        "outputId": "48ad7a96-b318-47d6-ca33-07545361f48a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "736/736 [==============================] - 66s 89ms/step - loss: 0.1599\n",
            "Epoch 2/5\n",
            "736/736 [==============================] - 66s 89ms/step - loss: 0.0590\n",
            "Epoch 3/5\n",
            "736/736 [==============================] - 65s 88ms/step - loss: 0.0403\n",
            "Epoch 4/5\n",
            "736/736 [==============================] - 66s 90ms/step - loss: 0.0302\n",
            "Epoch 5/5\n",
            "736/736 [==============================] - 65s 88ms/step - loss: 0.0230\n"
          ]
        }
      ],
      "source": [
        "# retrain the model using all data\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(85, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt,\n",
        "          loss='binary_crossentropy',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    trainval_s['X'],\n",
        "    trainval_s['att'],\n",
        "    epochs=best_epochs,\n",
        "    verbose=1,\n",
        ")\n",
        "model.save_weights(os.path.join(PATH_MODELS, 'awa_mlp_att_full'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MgCRqjEkK4U",
        "outputId": "ad05c467-f81a-4725-d1e6-75ead831e3eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fba47daef80>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# load the best model\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(85, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=opt,\n",
        "          loss='binary_crossentropy',\n",
        "          metrics=['accuracy'])\n",
        "model.load_weights(os.path.join(PATH_MODELS, 'awa_mlp_att_full'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ZXGGtdkYgyl1"
      },
      "outputs": [],
      "source": [
        "# get predictions of the attributes\n",
        "test_u_att_pred = model.predict(test_u['X'], verbose=0)\n",
        "test_s_att_pred = model.predict(test_s['X'], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkcV8eR2g7k1",
        "outputId": "d442f80c-d244-4b51-bc95-327af3a7d626"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           6      0.186     0.223     0.203      1645\n",
            "           8      0.175     0.960     0.295       174\n",
            "          22      0.403     0.592     0.480      1420\n",
            "          23      0.378     0.355     0.366       988\n",
            "          29      0.000     0.000     0.000       383\n",
            "          30      0.095     0.016     0.027      1202\n",
            "          33      0.430     0.952     0.592       310\n",
            "          40      0.686     0.851     0.760       630\n",
            "          46      0.167     0.098     0.123       215\n",
            "          49      0.152     0.029     0.048       946\n",
            "\n",
            "    accuracy                          0.332      7913\n",
            "   macro avg      0.267     0.407     0.289      7913\n",
            "weighted avg      0.271     0.332     0.277      7913\n",
            "\n",
            "[[ 367    0 1115    0    0  148    2   13    0    0]\n",
            " [   0  167    0    7    0    0    0    0    0    0]\n",
            " [ 435    0  841   16    0   17   65   43    3    0]\n",
            " [   7  296   44  351    0    3   46   14  101  126]\n",
            " [ 116    6   55    3    0    0  197    2    0    4]\n",
            " [1016    0    1    0    0   19    0  166    0    0]\n",
            " [   7    0    6    1    0    0  295    0    0    1]\n",
            " [  13    0    3    0    0    1   77  536    0    0]\n",
            " [   4   84   21   41    0   13    4    7   21   20]\n",
            " [   4  404    0  510    0    0    0    0    1   27]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# search for the nearest class based on the predicted attributes\n",
        "knn = KNN(1)\n",
        "knn.fit(knn_att[y_test_unseen], y_test_unseen)\n",
        "test_preds = knn.predict(test_u_att_pred)\n",
        "print(classification_report(test_u['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_u['y'], test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zalNS9FpZvIW",
        "outputId": "d46666c1-1c3e-489c-f6af-45be39ca0638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.949     0.918     0.933       182\n",
            "           1      0.939     0.951     0.945       162\n",
            "           2      0.922     0.983     0.952        60\n",
            "           3      0.838     0.756     0.795        41\n",
            "           4      0.963     0.972     0.967       107\n",
            "           5      0.950     0.899     0.924       148\n",
            "           7      0.937     0.890     0.913       218\n",
            "           9      0.853     0.930     0.890       100\n",
            "          10      1.000     0.968     0.984        31\n",
            "          11      0.833     0.800     0.816        25\n",
            "          12      1.000     1.000     1.000       175\n",
            "          13      0.926     0.972     0.948       141\n",
            "          14      1.000     1.000     1.000       152\n",
            "          15      0.854     0.933     0.892       150\n",
            "          16      0.818     0.667     0.735        54\n",
            "          17      0.979     0.958     0.968       144\n",
            "          18      0.984     0.989     0.987       189\n",
            "          19      0.975     0.963     0.969       163\n",
            "          20      0.941     0.793     0.860       140\n",
            "          21      0.984     0.984     0.984       124\n",
            "          24      0.874     0.944     0.908       162\n",
            "          25      0.900     0.971     0.934       139\n",
            "          26      0.963     0.987     0.975       239\n",
            "          27      0.992     0.941     0.966       135\n",
            "          28      0.986     1.000     0.993       211\n",
            "          31      0.949     0.941     0.945       119\n",
            "          32      0.882     0.882     0.882       119\n",
            "          34      0.884     0.760     0.817        50\n",
            "          35      0.906     0.934     0.920       166\n",
            "          36      0.959     0.959     0.959       197\n",
            "          37      1.000     1.000     1.000       228\n",
            "          38      1.000     0.995     0.997       182\n",
            "          39      0.935     0.949     0.942       275\n",
            "          41      0.986     0.916     0.950       155\n",
            "          42      0.995     0.990     0.992       198\n",
            "          43      0.889     0.600     0.716        40\n",
            "          44      1.000     0.989     0.995       184\n",
            "          45      0.912     0.945     0.928       219\n",
            "          47      0.929     0.875     0.901       104\n",
            "          48      0.862     0.961     0.909       254\n",
            "\n",
            "    accuracy                          0.945      5882\n",
            "   macro avg      0.936     0.922     0.927      5882\n",
            "weighted avg      0.945     0.945     0.944      5882\n",
            "\n",
            "[[167   1   0 ...   0   0   0]\n",
            " [  0 154   0 ...   0   0   0]\n",
            " [  0   0  59 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ... 207   0   2]\n",
            " [  0   0   0 ...   1  91   0]\n",
            " [  0   1   0 ...   1   0 244]]\n"
          ]
        }
      ],
      "source": [
        "# find the nearest neighbor based on the predicted attributes found in the image to infer the class\n",
        "knn = KNN(n_neighbors=1)\n",
        "msk = [x not in y_test_unseen for x in knn_y]\n",
        "knn.fit(knn_att[msk], knn_y[msk])\n",
        "# seen test classes\n",
        "test_preds = knn.predict(test_s_att_pred)\n",
        "print(classification_report(test_s['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_s['y'], test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQS5W02ijcSJ"
      },
      "source": [
        "## CUB Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "D7PYY6ySjWIz"
      },
      "outputs": [],
      "source": [
        "train, val, test_u, trainval_s, test_s, knn_att, knn_y, y_test_unseen, class_names = load_data('CUB', bin_att=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4j539doCjodk"
      },
      "outputs": [],
      "source": [
        "# split the trainval dataset\n",
        "train_X, val_X, train_att, val_att, train_y, val_y = train_test_split(trainval_s['X'], trainval_s['att'], trainval_s['y'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yzB2G0ejwiJ",
        "outputId": "0bd78f10-f1d6-475b-f74b-b9c62025937b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200, 312)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# it has too many attributes to create a separate model for each\n",
        "# we will train only joint model\n",
        "knn_att.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwyh2bu2aLzX"
      },
      "source": [
        "### Joint model for attribute prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7ThfRGwj56A",
        "outputId": "bbd40900-6804-4441-98e1-c178ccb61c8b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "177/177 [==============================] - 7s 12ms/step - loss: 3.3899 - val_loss: 1.9239\n",
            "Epoch 2/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 2.2819 - val_loss: 1.5650\n",
            "Epoch 3/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 2.0469 - val_loss: 1.4434\n",
            "Epoch 4/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 1.9995 - val_loss: 1.4175\n",
            "Epoch 5/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 1.9506 - val_loss: 1.3647\n",
            "Epoch 6/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 1.8467 - val_loss: 1.2626\n",
            "Epoch 7/300\n",
            "177/177 [==============================] - 3s 16ms/step - loss: 1.7998 - val_loss: 1.2475\n",
            "Epoch 8/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 1.7637 - val_loss: 1.2451\n",
            "Epoch 9/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 1.7277 - val_loss: 1.2148\n",
            "Epoch 10/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 1.6457 - val_loss: 1.1464\n",
            "Epoch 11/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 1.6079 - val_loss: 1.0727\n",
            "Epoch 12/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.6002 - val_loss: 1.0928\n",
            "Epoch 13/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.6156 - val_loss: 1.1164\n",
            "Epoch 14/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 1.5396 - val_loss: 1.0472\n",
            "Epoch 15/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.5293 - val_loss: 1.0889\n",
            "Epoch 16/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.5048 - val_loss: 1.0481\n",
            "Epoch 17/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 1.4842 - val_loss: 1.0053\n",
            "Epoch 18/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 1.3805 - val_loss: 0.9125\n",
            "Epoch 19/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 1.3281 - val_loss: 0.8902\n",
            "Epoch 20/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 1.2561 - val_loss: 0.8281\n",
            "Epoch 21/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 1.2033 - val_loss: 0.8370\n",
            "Epoch 22/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 1.1920 - val_loss: 0.8511\n",
            "Epoch 23/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.2232 - val_loss: 0.8982\n",
            "Epoch 24/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 1.2051 - val_loss: 0.8018\n",
            "Epoch 25/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 1.1835 - val_loss: 0.8078\n",
            "Epoch 26/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 1.1149 - val_loss: 0.7665\n",
            "Epoch 27/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.1102 - val_loss: 0.7919\n",
            "Epoch 28/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 1.0709 - val_loss: 0.7479\n",
            "Epoch 29/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 1.0343 - val_loss: 0.7361\n",
            "Epoch 30/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 1.0141 - val_loss: 0.7335\n",
            "Epoch 31/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.0606 - val_loss: 0.8265\n",
            "Epoch 32/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 1.0515 - val_loss: 0.7352\n",
            "Epoch 33/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 0.9970 - val_loss: 0.7280\n",
            "Epoch 34/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.9692 - val_loss: 0.6861\n",
            "Epoch 35/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 0.9268 - val_loss: 0.6579\n",
            "Epoch 36/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.9052 - val_loss: 0.6985\n",
            "Epoch 37/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.8841 - val_loss: 0.6608\n",
            "Epoch 38/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.8736 - val_loss: 0.6374\n",
            "Epoch 39/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.8547 - val_loss: 0.6228\n",
            "Epoch 40/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.8224 - val_loss: 0.6213\n",
            "Epoch 41/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.8021 - val_loss: 0.6061\n",
            "Epoch 42/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.7759 - val_loss: 0.6046\n",
            "Epoch 43/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.7632 - val_loss: 0.5998\n",
            "Epoch 44/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.7503 - val_loss: 0.5877\n",
            "Epoch 45/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.7577 - val_loss: 0.5949\n",
            "Epoch 46/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.7444 - val_loss: 0.5888\n",
            "Epoch 47/300\n",
            "177/177 [==============================] - 2s 11ms/step - loss: 0.7187 - val_loss: 0.5716\n",
            "Epoch 48/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.6981 - val_loss: 0.5615\n",
            "Epoch 49/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.6868 - val_loss: 0.5592\n",
            "Epoch 50/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.6801 - val_loss: 0.5687\n",
            "Epoch 51/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.6774 - val_loss: 0.5537\n",
            "Epoch 52/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.6600 - val_loss: 0.5512\n",
            "Epoch 53/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.6536 - val_loss: 0.5716\n",
            "Epoch 54/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.6628 - val_loss: 0.5556\n",
            "Epoch 55/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.6744 - val_loss: 0.5664\n",
            "Epoch 56/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.6584 - val_loss: 0.5587\n",
            "Epoch 57/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.6527 - val_loss: 0.5390\n",
            "Epoch 58/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.6595 - val_loss: 0.5580\n",
            "Epoch 59/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.6607 - val_loss: 0.5398\n",
            "Epoch 60/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.6298 - val_loss: 0.5345\n",
            "Epoch 61/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.6228 - val_loss: 0.5314\n",
            "Epoch 62/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.6346 - val_loss: 0.5427\n",
            "Epoch 63/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.6293 - val_loss: 0.5404\n",
            "Epoch 64/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.6056 - val_loss: 0.5292\n",
            "Epoch 65/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.6000 - val_loss: 0.5404\n",
            "Epoch 66/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.6141 - val_loss: 0.5269\n",
            "Epoch 67/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.5964 - val_loss: 0.5201\n",
            "Epoch 68/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.5847 - val_loss: 0.5188\n",
            "Epoch 69/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.5829 - val_loss: 0.5160\n",
            "Epoch 70/300\n",
            "177/177 [==============================] - 2s 8ms/step - loss: 0.5729 - val_loss: 0.5111\n",
            "Epoch 71/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.5878 - val_loss: 0.5343\n",
            "Epoch 72/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.5793 - val_loss: 0.5202\n",
            "Epoch 73/300\n",
            "177/177 [==============================] - 2s 11ms/step - loss: 0.5632 - val_loss: 0.5121\n",
            "Epoch 74/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.5562 - val_loss: 0.5203\n",
            "Epoch 75/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5548 - val_loss: 0.5120\n",
            "Epoch 76/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.5464 - val_loss: 0.5043\n",
            "Epoch 77/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5530 - val_loss: 0.5054\n",
            "Epoch 78/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5487 - val_loss: 0.5056\n",
            "Epoch 79/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.5394 - val_loss: 0.5006\n",
            "Epoch 80/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5333 - val_loss: 0.5110\n",
            "Epoch 81/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.5279 - val_loss: 0.4949\n",
            "Epoch 82/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.5248 - val_loss: 0.4902\n",
            "Epoch 83/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5202 - val_loss: 0.4910\n",
            "Epoch 84/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5181 - val_loss: 0.4993\n",
            "Epoch 85/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 0.5172 - val_loss: 0.4862\n",
            "Epoch 86/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.5126 - val_loss: 0.4805\n",
            "Epoch 87/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.5089 - val_loss: 0.4828\n",
            "Epoch 88/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.5053 - val_loss: 0.4803\n",
            "Epoch 89/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5033 - val_loss: 0.4806\n",
            "Epoch 90/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.5027 - val_loss: 0.4768\n",
            "Epoch 91/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5013 - val_loss: 0.5010\n",
            "Epoch 92/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.5006 - val_loss: 0.4789\n",
            "Epoch 93/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4947 - val_loss: 0.4762\n",
            "Epoch 94/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4949 - val_loss: 0.4894\n",
            "Epoch 95/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4961 - val_loss: 0.4873\n",
            "Epoch 96/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4929 - val_loss: 0.4905\n",
            "Epoch 97/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4926 - val_loss: 0.4765\n",
            "Epoch 98/300\n",
            "177/177 [==============================] - 2s 11ms/step - loss: 0.4870 - val_loss: 0.4696\n",
            "Epoch 99/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4843 - val_loss: 0.4721\n",
            "Epoch 100/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4823 - val_loss: 0.4719\n",
            "Epoch 101/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4766 - val_loss: 0.4700\n",
            "Epoch 102/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4749 - val_loss: 0.4639\n",
            "Epoch 103/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4717 - val_loss: 0.4611\n",
            "Epoch 104/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4697 - val_loss: 0.4696\n",
            "Epoch 105/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4666 - val_loss: 0.4497\n",
            "Epoch 106/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4698 - val_loss: 0.4653\n",
            "Epoch 107/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4635 - val_loss: 0.4500\n",
            "Epoch 108/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.4598 - val_loss: 0.4667\n",
            "Epoch 109/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4587 - val_loss: 0.4586\n",
            "Epoch 110/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4687 - val_loss: 0.4681\n",
            "Epoch 111/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4613 - val_loss: 0.4573\n",
            "Epoch 112/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4544 - val_loss: 0.4673\n",
            "Epoch 113/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.4514 - val_loss: 0.4487\n",
            "Epoch 114/300\n",
            "177/177 [==============================] - 2s 11ms/step - loss: 0.4476 - val_loss: 0.4472\n",
            "Epoch 115/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.4465 - val_loss: 0.4372\n",
            "Epoch 116/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4463 - val_loss: 0.4583\n",
            "Epoch 117/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.4422 - val_loss: 0.4506\n",
            "Epoch 118/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4373 - val_loss: 0.4364\n",
            "Epoch 119/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4461 - val_loss: 0.4460\n",
            "Epoch 120/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4405 - val_loss: 0.4445\n",
            "Epoch 121/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4430 - val_loss: 0.4448\n",
            "Epoch 122/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.4388 - val_loss: 0.4404\n",
            "Epoch 123/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4335 - val_loss: 0.4387\n",
            "Epoch 124/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.4293 - val_loss: 0.4348\n",
            "Epoch 125/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4306 - val_loss: 0.4467\n",
            "Epoch 126/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4348 - val_loss: 0.4341\n",
            "Epoch 127/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.4272 - val_loss: 0.4327\n",
            "Epoch 128/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 0.4219 - val_loss: 0.4286\n",
            "Epoch 129/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.4300 - val_loss: 0.4408\n",
            "Epoch 130/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4298 - val_loss: 0.4372\n",
            "Epoch 131/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4228 - val_loss: 0.4394\n",
            "Epoch 132/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4206 - val_loss: 0.4288\n",
            "Epoch 133/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4225 - val_loss: 0.4496\n",
            "Epoch 134/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4262 - val_loss: 0.4393\n",
            "Epoch 135/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4196 - val_loss: 0.4306\n",
            "Epoch 136/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4174 - val_loss: 0.4355\n",
            "Epoch 137/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4157 - val_loss: 0.4271\n",
            "Epoch 138/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.4104 - val_loss: 0.4302\n",
            "Epoch 139/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.4057 - val_loss: 0.4238\n",
            "Epoch 140/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.4066 - val_loss: 0.4285\n",
            "Epoch 141/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.4015 - val_loss: 0.4169\n",
            "Epoch 142/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.4000 - val_loss: 0.4242\n",
            "Epoch 143/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3977 - val_loss: 0.4282\n",
            "Epoch 144/300\n",
            "177/177 [==============================] - 2s 10ms/step - loss: 0.3984 - val_loss: 0.4149\n",
            "Epoch 145/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3937 - val_loss: 0.4202\n",
            "Epoch 146/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3929 - val_loss: 0.4220\n",
            "Epoch 147/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3910 - val_loss: 0.4183\n",
            "Epoch 148/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3869 - val_loss: 0.4204\n",
            "Epoch 149/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.3847 - val_loss: 0.4047\n",
            "Epoch 150/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3911 - val_loss: 0.4127\n",
            "Epoch 151/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3916 - val_loss: 0.4174\n",
            "Epoch 152/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3939 - val_loss: 0.4190\n",
            "Epoch 153/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3981 - val_loss: 0.4280\n",
            "Epoch 154/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3901 - val_loss: 0.4140\n",
            "Epoch 155/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3856 - val_loss: 0.4146\n",
            "Epoch 156/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3819 - val_loss: 0.4091\n",
            "Epoch 157/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3824 - val_loss: 0.4098\n",
            "Epoch 158/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3773 - val_loss: 0.4066\n",
            "Epoch 159/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3775 - val_loss: 0.4071\n",
            "Epoch 160/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3764 - val_loss: 0.4082\n",
            "Epoch 161/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3732 - val_loss: 0.4027\n",
            "Epoch 162/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3707 - val_loss: 0.4101\n",
            "Epoch 163/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3698 - val_loss: 0.4039\n",
            "Epoch 164/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.3642 - val_loss: 0.3928\n",
            "Epoch 165/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3602 - val_loss: 0.3947\n",
            "Epoch 166/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3585 - val_loss: 0.4034\n",
            "Epoch 167/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3625 - val_loss: 0.3955\n",
            "Epoch 168/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3598 - val_loss: 0.3968\n",
            "Epoch 169/300\n",
            "177/177 [==============================] - 1s 4ms/step - loss: 0.3598 - val_loss: 0.3928\n",
            "Epoch 170/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3543 - val_loss: 0.3903\n",
            "Epoch 171/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3505 - val_loss: 0.3970\n",
            "Epoch 172/300\n",
            "177/177 [==============================] - 2s 8ms/step - loss: 0.3503 - val_loss: 0.3895\n",
            "Epoch 173/300\n",
            "177/177 [==============================] - 2s 11ms/step - loss: 0.3483 - val_loss: 0.3840\n",
            "Epoch 174/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3452 - val_loss: 0.3875\n",
            "Epoch 175/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3464 - val_loss: 0.3866\n",
            "Epoch 176/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3460 - val_loss: 0.3836\n",
            "Epoch 177/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.3416 - val_loss: 0.3823\n",
            "Epoch 178/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3378 - val_loss: 0.3773\n",
            "Epoch 179/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3348 - val_loss: 0.3778\n",
            "Epoch 180/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3332 - val_loss: 0.3737\n",
            "Epoch 181/300\n",
            "177/177 [==============================] - 2s 8ms/step - loss: 0.3301 - val_loss: 0.3732\n",
            "Epoch 182/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3280 - val_loss: 0.3949\n",
            "Epoch 183/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.3341 - val_loss: 0.3702\n",
            "Epoch 184/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3302 - val_loss: 0.3781\n",
            "Epoch 185/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3251 - val_loss: 0.3733\n",
            "Epoch 186/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3376 - val_loss: 0.3829\n",
            "Epoch 187/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3327 - val_loss: 0.3756\n",
            "Epoch 188/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3282 - val_loss: 0.3971\n",
            "Epoch 189/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3263 - val_loss: 0.3763\n",
            "Epoch 190/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3219 - val_loss: 0.3723\n",
            "Epoch 191/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3278 - val_loss: 0.3986\n",
            "Epoch 192/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3323 - val_loss: 0.3828\n",
            "Epoch 193/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3310 - val_loss: 0.3841\n",
            "Epoch 194/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3339 - val_loss: 0.3785\n",
            "Epoch 195/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3277 - val_loss: 0.3827\n",
            "Epoch 196/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3263 - val_loss: 0.3975\n",
            "Epoch 197/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3476 - val_loss: 0.3947\n",
            "Epoch 198/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3409 - val_loss: 0.3857\n",
            "Epoch 199/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3311 - val_loss: 0.3861\n",
            "Epoch 200/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3275 - val_loss: 0.3784\n",
            "Epoch 201/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3231 - val_loss: 0.3730\n",
            "Epoch 202/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3188 - val_loss: 0.3730\n",
            "Epoch 203/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.3154 - val_loss: 0.3678\n",
            "Epoch 204/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3121 - val_loss: 0.3729\n",
            "Epoch 205/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3127 - val_loss: 0.3714\n",
            "Epoch 206/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3311 - val_loss: 0.3840\n",
            "Epoch 207/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3256 - val_loss: 0.3769\n",
            "Epoch 208/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3196 - val_loss: 0.3739\n",
            "Epoch 209/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3408 - val_loss: 0.3843\n",
            "Epoch 210/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3359 - val_loss: 0.3847\n",
            "Epoch 211/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3271 - val_loss: 0.3844\n",
            "Epoch 212/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3219 - val_loss: 0.3784\n",
            "Epoch 213/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3267 - val_loss: 0.3799\n",
            "Epoch 214/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3214 - val_loss: 0.3833\n",
            "Epoch 215/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.3160 - val_loss: 0.3731\n",
            "Epoch 216/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.3140 - val_loss: 0.3810\n",
            "Epoch 217/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.3112 - val_loss: 0.3713\n",
            "Epoch 218/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3079 - val_loss: 0.3757\n",
            "Epoch 219/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3094 - val_loss: 0.3700\n",
            "Epoch 220/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3122 - val_loss: 0.3649\n",
            "Epoch 221/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.3069 - val_loss: 0.3665\n",
            "Epoch 222/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.3014 - val_loss: 0.3642\n",
            "Epoch 223/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2970 - val_loss: 0.3635\n",
            "Epoch 224/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2962 - val_loss: 0.3656\n",
            "Epoch 225/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2955 - val_loss: 0.3547\n",
            "Epoch 226/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2923 - val_loss: 0.3577\n",
            "Epoch 227/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.2886 - val_loss: 0.3516\n",
            "Epoch 228/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2875 - val_loss: 0.3907\n",
            "Epoch 229/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2982 - val_loss: 0.3670\n",
            "Epoch 230/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2954 - val_loss: 0.3588\n",
            "Epoch 231/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2957 - val_loss: 0.3614\n",
            "Epoch 232/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2886 - val_loss: 0.3584\n",
            "Epoch 233/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2838 - val_loss: 0.3547\n",
            "Epoch 234/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2859 - val_loss: 0.3420\n",
            "Epoch 235/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2965 - val_loss: 0.3551\n",
            "Epoch 236/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2870 - val_loss: 0.3490\n",
            "Epoch 237/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2825 - val_loss: 0.3474\n",
            "Epoch 238/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2839 - val_loss: 0.3515\n",
            "Epoch 239/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2807 - val_loss: 0.3457\n",
            "Epoch 240/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2790 - val_loss: 0.3493\n",
            "Epoch 241/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2747 - val_loss: 0.3551\n",
            "Epoch 242/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2707 - val_loss: 0.3389\n",
            "Epoch 243/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2783 - val_loss: 0.3475\n",
            "Epoch 244/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2719 - val_loss: 0.3549\n",
            "Epoch 245/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2704 - val_loss: 0.3497\n",
            "Epoch 246/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2674 - val_loss: 0.3505\n",
            "Epoch 247/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2671 - val_loss: 0.3634\n",
            "Epoch 248/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2645 - val_loss: 0.3455\n",
            "Epoch 249/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2645 - val_loss: 0.3399\n",
            "Epoch 250/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2628 - val_loss: 0.3447\n",
            "Epoch 251/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2582 - val_loss: 0.3398\n",
            "Epoch 252/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.2537 - val_loss: 0.3383\n",
            "Epoch 253/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2507 - val_loss: 0.3502\n",
            "Epoch 254/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2545 - val_loss: 0.3417\n",
            "Epoch 255/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2537 - val_loss: 0.3536\n",
            "Epoch 256/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2510 - val_loss: 0.3317\n",
            "Epoch 257/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2485 - val_loss: 0.3377\n",
            "Epoch 258/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2458 - val_loss: 0.3427\n",
            "Epoch 259/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2421 - val_loss: 0.3375\n",
            "Epoch 260/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2426 - val_loss: 0.3391\n",
            "Epoch 261/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2502 - val_loss: 0.3381\n",
            "Epoch 262/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2542 - val_loss: 0.3474\n",
            "Epoch 263/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2489 - val_loss: 0.3329\n",
            "Epoch 264/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2454 - val_loss: 0.3396\n",
            "Epoch 265/300\n",
            "177/177 [==============================] - 2s 9ms/step - loss: 0.2466 - val_loss: 0.3305\n",
            "Epoch 266/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2439 - val_loss: 0.3218\n",
            "Epoch 267/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2483 - val_loss: 0.3337\n",
            "Epoch 268/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2433 - val_loss: 0.3216\n",
            "Epoch 269/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2540 - val_loss: 0.3482\n",
            "Epoch 270/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2447 - val_loss: 0.3331\n",
            "Epoch 271/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2404 - val_loss: 0.3270\n",
            "Epoch 272/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2373 - val_loss: 0.3251\n",
            "Epoch 273/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2348 - val_loss: 0.3276\n",
            "Epoch 274/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2312 - val_loss: 0.3264\n",
            "Epoch 275/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2262 - val_loss: 0.3280\n",
            "Epoch 276/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2263 - val_loss: 0.3378\n",
            "Epoch 277/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2233 - val_loss: 0.3260\n",
            "Epoch 278/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.2202 - val_loss: 0.3348\n",
            "Epoch 279/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.2185 - val_loss: 0.3384\n",
            "Epoch 280/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2181 - val_loss: 0.3306\n",
            "Epoch 281/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2171 - val_loss: 0.3239\n",
            "Epoch 282/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2161 - val_loss: 0.3243\n",
            "Epoch 283/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2129 - val_loss: 0.3238\n",
            "Epoch 284/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2097 - val_loss: 0.3245\n",
            "Epoch 285/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2072 - val_loss: 0.3184\n",
            "Epoch 286/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.3253\n",
            "Epoch 287/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2072 - val_loss: 0.3301\n",
            "Epoch 288/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2043 - val_loss: 0.3189\n",
            "Epoch 289/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.3232\n",
            "Epoch 290/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.2097 - val_loss: 0.3174\n",
            "Epoch 291/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.2049 - val_loss: 0.3262\n",
            "Epoch 292/300\n",
            "177/177 [==============================] - 1s 7ms/step - loss: 0.1992 - val_loss: 0.3182\n",
            "Epoch 293/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.1958 - val_loss: 0.3156\n",
            "Epoch 294/300\n",
            "177/177 [==============================] - 1s 8ms/step - loss: 0.1946 - val_loss: 0.3149\n",
            "Epoch 295/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.1938 - val_loss: 0.3155\n",
            "Epoch 296/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.1899 - val_loss: 0.3171\n",
            "Epoch 297/300\n",
            "177/177 [==============================] - 1s 6ms/step - loss: 0.1904 - val_loss: 0.3094\n",
            "Epoch 298/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.1900 - val_loss: 0.3224\n",
            "Epoch 299/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.1865 - val_loss: 0.3163\n",
            "Epoch 300/300\n",
            "177/177 [==============================] - 1s 5ms/step - loss: 0.1842 - val_loss: 0.3334\n"
          ]
        }
      ],
      "source": [
        "# joint model\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(312))\n",
        "\n",
        "opt = Adam(learning_rate=0.000025)\n",
        "model.compile(optimizer=opt,\n",
        "          loss='binary_crossentropy',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_X,\n",
        "    train_att,\n",
        "    validation_data=(val_X, val_att),\n",
        "    epochs=300,\n",
        "    verbose=1,\n",
        "    callbacks=[tf.keras.callbacks.ModelCheckpoint(os.path.join(PATH_MODELS, 'cub_mlp_att_full_VAL'), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnoYsrjFOswt"
      },
      "outputs": [],
      "source": [
        "best_epochs = np.argmin(pd.DataFrame(history.history).val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUrJBylTrOe8",
        "outputId": "08592575-4049-4666-dcf1-33953b646d51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/296\n",
            "221/221 [==============================] - 6s 8ms/step - loss: 0.5007\n",
            "Epoch 2/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.4292\n",
            "Epoch 3/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3986\n",
            "Epoch 4/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3771\n",
            "Epoch 5/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3593\n",
            "Epoch 6/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3439\n",
            "Epoch 7/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3301\n",
            "Epoch 8/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3170\n",
            "Epoch 9/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.3047\n",
            "Epoch 10/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.2934\n",
            "Epoch 11/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.2824\n",
            "Epoch 12/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2726\n",
            "Epoch 13/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2629\n",
            "Epoch 14/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2541\n",
            "Epoch 15/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2450\n",
            "Epoch 16/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2365\n",
            "Epoch 17/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2286\n",
            "Epoch 18/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2209\n",
            "Epoch 19/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.2128\n",
            "Epoch 20/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.2057\n",
            "Epoch 21/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.1990\n",
            "Epoch 22/296\n",
            "221/221 [==============================] - 1s 7ms/step - loss: 0.1918\n",
            "Epoch 23/296\n",
            "221/221 [==============================] - 2s 7ms/step - loss: 0.1861\n",
            "Epoch 24/296\n",
            "221/221 [==============================] - 2s 8ms/step - loss: 0.1797\n",
            "Epoch 25/296\n",
            "221/221 [==============================] - 2s 8ms/step - loss: 0.1734\n",
            "Epoch 26/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.1679\n",
            "Epoch 27/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.1620\n",
            "Epoch 28/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.1568\n",
            "Epoch 29/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1507\n",
            "Epoch 30/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1455\n",
            "Epoch 31/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1408\n",
            "Epoch 32/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1357\n",
            "Epoch 33/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1309\n",
            "Epoch 34/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1267\n",
            "Epoch 35/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1219\n",
            "Epoch 36/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1174\n",
            "Epoch 37/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.1128\n",
            "Epoch 38/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.1090\n",
            "Epoch 39/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.1049\n",
            "Epoch 40/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.1009\n",
            "Epoch 41/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0976\n",
            "Epoch 42/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0937\n",
            "Epoch 43/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0901\n",
            "Epoch 44/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0867\n",
            "Epoch 45/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0829\n",
            "Epoch 46/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0798\n",
            "Epoch 47/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0770\n",
            "Epoch 48/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0735\n",
            "Epoch 49/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0707\n",
            "Epoch 50/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0675\n",
            "Epoch 51/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0649\n",
            "Epoch 52/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0621\n",
            "Epoch 53/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0601\n",
            "Epoch 54/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0567\n",
            "Epoch 55/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0543\n",
            "Epoch 56/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0524\n",
            "Epoch 57/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0498\n",
            "Epoch 58/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0481\n",
            "Epoch 59/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0456\n",
            "Epoch 60/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0438\n",
            "Epoch 61/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0418\n",
            "Epoch 62/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0396\n",
            "Epoch 63/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0384\n",
            "Epoch 64/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0367\n",
            "Epoch 65/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0348\n",
            "Epoch 66/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0332\n",
            "Epoch 67/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0318\n",
            "Epoch 68/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0304\n",
            "Epoch 69/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0288\n",
            "Epoch 70/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0280\n",
            "Epoch 71/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0268\n",
            "Epoch 72/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0255\n",
            "Epoch 73/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0241\n",
            "Epoch 74/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0230\n",
            "Epoch 75/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0220\n",
            "Epoch 76/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0209\n",
            "Epoch 77/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0201\n",
            "Epoch 78/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0192\n",
            "Epoch 79/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0185\n",
            "Epoch 80/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0174\n",
            "Epoch 81/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0168\n",
            "Epoch 82/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0163\n",
            "Epoch 83/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0154\n",
            "Epoch 84/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0147\n",
            "Epoch 85/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0139\n",
            "Epoch 86/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0134\n",
            "Epoch 87/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0128\n",
            "Epoch 88/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0121\n",
            "Epoch 89/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0117\n",
            "Epoch 90/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0112\n",
            "Epoch 91/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0107\n",
            "Epoch 92/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0104\n",
            "Epoch 93/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0101\n",
            "Epoch 94/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0096\n",
            "Epoch 95/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0090\n",
            "Epoch 96/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0088\n",
            "Epoch 97/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0083\n",
            "Epoch 98/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0080\n",
            "Epoch 99/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0078\n",
            "Epoch 100/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0075\n",
            "Epoch 101/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0071\n",
            "Epoch 102/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0068\n",
            "Epoch 103/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0067\n",
            "Epoch 104/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0064\n",
            "Epoch 105/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0060\n",
            "Epoch 106/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0059\n",
            "Epoch 107/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0058\n",
            "Epoch 108/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0055\n",
            "Epoch 109/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0054\n",
            "Epoch 110/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0050\n",
            "Epoch 111/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0050\n",
            "Epoch 112/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0048\n",
            "Epoch 113/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0046\n",
            "Epoch 114/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0044\n",
            "Epoch 115/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0042\n",
            "Epoch 116/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0041\n",
            "Epoch 117/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0040\n",
            "Epoch 118/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0039\n",
            "Epoch 119/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0038\n",
            "Epoch 120/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0036\n",
            "Epoch 121/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0036\n",
            "Epoch 122/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0035\n",
            "Epoch 123/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0034\n",
            "Epoch 124/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0032\n",
            "Epoch 125/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0031\n",
            "Epoch 126/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0030\n",
            "Epoch 127/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0029\n",
            "Epoch 128/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0028\n",
            "Epoch 129/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0027\n",
            "Epoch 130/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0026\n",
            "Epoch 131/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0026\n",
            "Epoch 132/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0025\n",
            "Epoch 133/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0025\n",
            "Epoch 134/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0023\n",
            "Epoch 135/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0023\n",
            "Epoch 136/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0022\n",
            "Epoch 137/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 138/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 139/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 140/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 141/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0021\n",
            "Epoch 142/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 143/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0019\n",
            "Epoch 144/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0018\n",
            "Epoch 145/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 146/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 147/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 148/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0017\n",
            "Epoch 149/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0016\n",
            "Epoch 150/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0016\n",
            "Epoch 151/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0015\n",
            "Epoch 152/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 0.0015\n",
            "Epoch 153/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0016\n",
            "Epoch 154/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 155/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 156/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0014\n",
            "Epoch 157/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 158/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 159/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 160/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0013\n",
            "Epoch 161/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 162/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 163/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0012\n",
            "Epoch 164/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0011\n",
            "Epoch 165/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 166/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 167/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 168/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0011\n",
            "Epoch 169/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 0.0010\n",
            "Epoch 170/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 171/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 172/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 0.0010\n",
            "Epoch 173/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 9.8229e-04\n",
            "Epoch 174/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 9.5783e-04\n",
            "Epoch 175/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 9.3336e-04\n",
            "Epoch 176/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 9.4182e-04\n",
            "Epoch 177/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 8.7134e-04\n",
            "Epoch 178/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 8.8539e-04\n",
            "Epoch 179/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 8.8156e-04\n",
            "Epoch 180/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 8.9904e-04\n",
            "Epoch 181/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 8.7289e-04\n",
            "Epoch 182/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 8.3482e-04\n",
            "Epoch 183/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 8.2771e-04\n",
            "Epoch 184/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 8.1739e-04\n",
            "Epoch 185/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 9.4562e-04\n",
            "Epoch 186/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 8.4248e-04\n",
            "Epoch 187/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 8.0429e-04\n",
            "Epoch 188/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 7.6637e-04\n",
            "Epoch 189/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 6.8763e-04\n",
            "Epoch 190/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 6.8858e-04\n",
            "Epoch 191/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 7.0453e-04\n",
            "Epoch 192/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 6.8594e-04\n",
            "Epoch 193/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 7.0847e-04\n",
            "Epoch 194/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 6.5689e-04\n",
            "Epoch 195/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 6.3195e-04\n",
            "Epoch 196/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 6.4169e-04\n",
            "Epoch 197/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 6.3764e-04\n",
            "Epoch 198/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 6.7221e-04\n",
            "Epoch 199/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 6.3691e-04\n",
            "Epoch 200/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 6.2486e-04\n",
            "Epoch 201/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 6.4943e-04\n",
            "Epoch 202/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 6.3651e-04\n",
            "Epoch 203/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.8106e-04\n",
            "Epoch 204/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.6790e-04\n",
            "Epoch 205/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.5185e-04\n",
            "Epoch 206/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.7035e-04\n",
            "Epoch 207/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.6596e-04\n",
            "Epoch 208/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.5500e-04\n",
            "Epoch 209/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.4333e-04\n",
            "Epoch 210/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.4373e-04\n",
            "Epoch 211/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.6521e-04\n",
            "Epoch 212/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.0625e-04\n",
            "Epoch 213/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.4910e-04\n",
            "Epoch 214/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 5.3904e-04\n",
            "Epoch 215/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 4.7795e-04\n",
            "Epoch 216/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 4.9082e-04\n",
            "Epoch 217/296\n",
            "221/221 [==============================] - 1s 7ms/step - loss: 4.6717e-04\n",
            "Epoch 218/296\n",
            "221/221 [==============================] - 2s 7ms/step - loss: 4.6017e-04\n",
            "Epoch 219/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 5.1085e-04\n",
            "Epoch 220/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.6213e-04\n",
            "Epoch 221/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.4267e-04\n",
            "Epoch 222/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.5603e-04\n",
            "Epoch 223/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.3593e-04\n",
            "Epoch 224/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.7714e-04\n",
            "Epoch 225/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.5341e-04\n",
            "Epoch 226/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.4481e-04\n",
            "Epoch 227/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.3451e-04\n",
            "Epoch 228/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.3188e-04\n",
            "Epoch 229/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.1053e-04\n",
            "Epoch 230/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 4.1638e-04\n",
            "Epoch 231/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.9025e-04\n",
            "Epoch 232/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.8967e-04\n",
            "Epoch 233/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.8882e-04\n",
            "Epoch 234/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 4.0788e-04\n",
            "Epoch 235/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.0674e-04\n",
            "Epoch 236/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.9485e-04\n",
            "Epoch 237/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.9577e-04\n",
            "Epoch 238/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.6253e-04\n",
            "Epoch 239/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.8692e-04\n",
            "Epoch 240/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 4.2652e-04\n",
            "Epoch 241/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.8349e-04\n",
            "Epoch 242/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.6369e-04\n",
            "Epoch 243/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.5980e-04\n",
            "Epoch 244/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.5608e-04\n",
            "Epoch 245/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.6636e-04\n",
            "Epoch 246/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.5728e-04\n",
            "Epoch 247/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.2200e-04\n",
            "Epoch 248/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.4132e-04\n",
            "Epoch 249/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 3.2877e-04\n",
            "Epoch 250/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 3.5452e-04\n",
            "Epoch 251/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.5784e-04\n",
            "Epoch 252/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.0188e-04\n",
            "Epoch 253/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.0080e-04\n",
            "Epoch 254/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.2603e-04\n",
            "Epoch 255/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.4477e-04\n",
            "Epoch 256/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.2562e-04\n",
            "Epoch 257/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.9443e-04\n",
            "Epoch 258/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.1970e-04\n",
            "Epoch 259/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.8614e-04\n",
            "Epoch 260/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.1143e-04\n",
            "Epoch 261/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.8562e-04\n",
            "Epoch 262/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.9986e-04\n",
            "Epoch 263/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.9242e-04\n",
            "Epoch 264/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.9138e-04\n",
            "Epoch 265/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.6314e-04\n",
            "Epoch 266/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 3.0487e-04\n",
            "Epoch 267/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.9365e-04\n",
            "Epoch 268/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.9659e-04\n",
            "Epoch 269/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.7264e-04\n",
            "Epoch 270/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.7550e-04\n",
            "Epoch 271/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.7933e-04\n",
            "Epoch 272/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.9359e-04\n",
            "Epoch 273/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.0343e-04\n",
            "Epoch 274/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.9729e-04\n",
            "Epoch 275/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 3.0594e-04\n",
            "Epoch 276/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.8497e-04\n",
            "Epoch 277/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.4479e-04\n",
            "Epoch 278/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.6830e-04\n",
            "Epoch 279/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.5074e-04\n",
            "Epoch 280/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.4697e-04\n",
            "Epoch 281/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.3214e-04\n",
            "Epoch 282/296\n",
            "221/221 [==============================] - 1s 6ms/step - loss: 2.3197e-04\n",
            "Epoch 283/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.4631e-04\n",
            "Epoch 284/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.5743e-04\n",
            "Epoch 285/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.5122e-04\n",
            "Epoch 286/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.6386e-04\n",
            "Epoch 287/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.3785e-04\n",
            "Epoch 288/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.3433e-04\n",
            "Epoch 289/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.7265e-04\n",
            "Epoch 290/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.4338e-04\n",
            "Epoch 291/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.2622e-04\n",
            "Epoch 292/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.3592e-04\n",
            "Epoch 293/296\n",
            "221/221 [==============================] - 1s 4ms/step - loss: 2.4274e-04\n",
            "Epoch 294/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.0707e-04\n",
            "Epoch 295/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.0836e-04\n",
            "Epoch 296/296\n",
            "221/221 [==============================] - 1s 5ms/step - loss: 2.1154e-04\n"
          ]
        }
      ],
      "source": [
        "# retrain the model using all data\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(312, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.00005)\n",
        "model.compile(optimizer=opt,\n",
        "          loss='binary_crossentropy',\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    trainval_s['X'],\n",
        "    trainval_s['att'],\n",
        "    epochs=best_epochs,\n",
        "    verbose=1,\n",
        ")\n",
        "model.save_weights(os.path.join(PATH_MODELS, 'cub_mlp_att_full'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBQ_usB5kv4S",
        "outputId": "99bf2ef9-95a9-42bb-cad6-4e96da34162f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fba47ff5b10>"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "# load the best model\n",
        "K.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Dense(2048, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(312, activation='sigmoid'))\n",
        "\n",
        "opt = Adam(learning_rate=0.00005)\n",
        "model.compile(optimizer=opt,\n",
        "          loss='binary_crossentropy',\n",
        ")\n",
        "model.load_weights(os.path.join(PATH_MODELS, 'cub_mlp_att_full'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "kgB3r7G1kxv6"
      },
      "outputs": [],
      "source": [
        "# get predictions of the attributes\n",
        "test_u_att_pred = model.predict(test_u['X'], verbose=0)\n",
        "test_s_att_pred = model.predict(test_s['X'], verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5U27h8xTkzrv",
        "outputId": "7d66c2ba-82d9-4855-ef4e-356e3b7c7530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           6      0.355     0.589     0.443        56\n",
            "          18      0.000     0.000     0.000        60\n",
            "          20      0.000     0.000     0.000        59\n",
            "          28      0.135     0.850     0.233        60\n",
            "          33      0.790     0.817     0.803        60\n",
            "          35      0.054     0.033     0.041        60\n",
            "          49      0.500     0.067     0.118        60\n",
            "          55      0.040     0.033     0.036        60\n",
            "          61      0.000     0.000     0.000        60\n",
            "          67      0.159     0.383     0.224        60\n",
            "          68      0.261     0.390     0.313        59\n",
            "          71      0.008     0.033     0.014        60\n",
            "          78      0.000     0.000     0.000        60\n",
            "          79      0.000     0.000     0.000        50\n",
            "          86      0.000     0.000     0.000        60\n",
            "          87      0.220     0.492     0.304        59\n",
            "          90      0.128     0.550     0.208        60\n",
            "          94      0.000     0.000     0.000        60\n",
            "          97      1.000     0.050     0.095        60\n",
            "          99      0.936     0.733     0.822        60\n",
            "         103      0.000     0.000     0.000        60\n",
            "         107      0.000     0.000     0.000        60\n",
            "         115      0.154     0.068     0.094        59\n",
            "         119      0.000     0.000     0.000        60\n",
            "         121      0.000     0.000     0.000        60\n",
            "         123      0.339     0.356     0.347        59\n",
            "         124      0.000     0.000     0.000        59\n",
            "         128      0.167     0.017     0.030        60\n",
            "         138      0.000     0.000     0.000        60\n",
            "         140      0.219     0.983     0.358        58\n",
            "         141      1.000     0.100     0.182        60\n",
            "         149      0.105     0.133     0.118        60\n",
            "         151      0.367     0.667     0.473        60\n",
            "         156      0.723     0.576     0.642        59\n",
            "         158      0.000     0.000     0.000        60\n",
            "         159      0.000     0.000     0.000        59\n",
            "         165      0.119     0.390     0.182        59\n",
            "         166      0.278     0.167     0.208        60\n",
            "         170      0.298     0.617     0.402        60\n",
            "         173      0.000     0.000     0.000        53\n",
            "         175      0.067     0.017     0.027        60\n",
            "         178      0.000     0.000     0.000        60\n",
            "         181      0.031     0.017     0.022        60\n",
            "         184      0.294     0.254     0.273        59\n",
            "         186      0.500     0.050     0.091        60\n",
            "         188      0.048     0.033     0.039        60\n",
            "         190      0.123     0.233     0.161        60\n",
            "         191      0.000     0.000     0.000        60\n",
            "         192      0.000     0.000     0.000        60\n",
            "         194      0.550     0.183     0.275        60\n",
            "\n",
            "    accuracy                          0.198      2967\n",
            "   macro avg      0.199     0.198     0.152      2967\n",
            "weighted avg      0.200     0.198     0.152      2967\n",
            "\n",
            "[[33  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  3 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0 11]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "# find the nearest neighbor based on the predicted attributes found in the image to infer the class\n",
        "knn = KNN(n_neighbors=1)\n",
        "knn.fit(knn_att[y_test_unseen], y_test_unseen)\n",
        "# search for the nearest class based on the predicted attributes\n",
        "# unseen test classes\n",
        "test_preds = knn.predict(test_u_att_pred)\n",
        "print(classification_report(test_u['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_u['y'], test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2A6YOGmnx4N",
        "outputId": "7168e298-3011-4d34-901f-213bf2f8fafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.706     0.800     0.750        15\n",
            "           1      0.789     0.750     0.769        20\n",
            "           2      1.000     0.857     0.923         7\n",
            "           3      0.917     0.846     0.880        13\n",
            "           4      1.000     0.857     0.923        14\n",
            "           5      0.625     0.417     0.500        12\n",
            "           7      0.778     0.875     0.824         8\n",
            "           8      0.875     0.875     0.875         8\n",
            "           9      1.000     1.000     1.000         7\n",
            "          10      0.800     0.800     0.800         5\n",
            "          11      1.000     0.923     0.960        13\n",
            "          12      0.875     0.636     0.737        11\n",
            "          13      0.917     0.786     0.846        14\n",
            "          14      0.867     0.867     0.867        15\n",
            "          15      0.786     0.786     0.786        14\n",
            "          16      1.000     0.857     0.923         7\n",
            "          17      0.857     0.750     0.800         8\n",
            "          19      0.636     0.700     0.667        10\n",
            "          21      0.444     0.333     0.381        12\n",
            "          22      0.500     0.286     0.364         7\n",
            "          23      0.857     0.400     0.545        15\n",
            "          24      0.636     0.500     0.560        14\n",
            "          25      0.833     0.769     0.800        13\n",
            "          26      0.833     0.909     0.870        11\n",
            "          27      0.667     0.667     0.667         9\n",
            "          29      0.818     1.000     0.900         9\n",
            "          30      0.909     1.000     0.952        10\n",
            "          31      1.000     0.786     0.880        14\n",
            "          32      0.833     0.714     0.769        14\n",
            "          34      0.833     0.833     0.833        12\n",
            "          36      0.812     1.000     0.897        13\n",
            "          37      1.000     1.000     1.000        12\n",
            "          38      0.818     1.000     0.900         9\n",
            "          39      0.200     0.250     0.222        12\n",
            "          40      0.429     0.273     0.333        11\n",
            "          41      0.765     0.867     0.812        15\n",
            "          42      0.267     0.333     0.296        12\n",
            "          43      0.818     0.818     0.818        11\n",
            "          44      0.545     0.429     0.480        14\n",
            "          45      0.667     0.500     0.571        12\n",
            "          46      0.909     0.769     0.833        13\n",
            "          47      0.600     0.750     0.667         8\n",
            "          48      0.833     0.833     0.833        12\n",
            "          50      0.733     1.000     0.846        11\n",
            "          51      1.000     1.000     1.000         9\n",
            "          52      1.000     0.857     0.923         7\n",
            "          53      1.000     1.000     1.000        10\n",
            "          54      0.875     0.933     0.903        15\n",
            "          56      1.000     0.500     0.667        12\n",
            "          57      0.923     1.000     0.960        12\n",
            "          58      0.933     0.933     0.933        15\n",
            "          59      0.800     0.571     0.667         7\n",
            "          60      0.833     0.769     0.800        13\n",
            "          62      1.000     1.000     1.000        14\n",
            "          63      0.875     1.000     0.933         7\n",
            "          64      0.917     0.917     0.917        12\n",
            "          65      0.733     0.786     0.759        14\n",
            "          66      0.556     0.909     0.690        11\n",
            "          69      1.000     0.818     0.900        11\n",
            "          70      0.867     1.000     0.929        13\n",
            "          72      0.538     0.500     0.519        14\n",
            "          73      1.000     0.846     0.917        13\n",
            "          74      0.357     0.714     0.476         7\n",
            "          75      0.833     0.625     0.714        16\n",
            "          76      0.750     0.818     0.783        11\n",
            "          77      0.667     1.000     0.800         6\n",
            "          80      0.000     0.000     0.000        13\n",
            "          81      0.429     0.333     0.375         9\n",
            "          82      0.556     0.417     0.476        12\n",
            "          83      0.800     0.727     0.762        11\n",
            "          84      0.800     0.667     0.727        12\n",
            "          85      0.733     0.647     0.688        17\n",
            "          88      0.375     0.600     0.462         5\n",
            "          89      0.889     0.500     0.640        16\n",
            "          91      0.615     0.889     0.727         9\n",
            "          92      0.714     0.667     0.690        15\n",
            "          93      0.333     0.429     0.375        14\n",
            "          95      0.800     0.800     0.800        15\n",
            "          96      1.000     0.933     0.966        15\n",
            "          98      0.444     0.333     0.381        12\n",
            "         100      0.889     0.727     0.800        11\n",
            "         101      0.609     0.824     0.700        17\n",
            "         102      0.750     0.692     0.720        13\n",
            "         104      0.500     0.286     0.364        14\n",
            "         105      0.462     0.600     0.522        10\n",
            "         106      0.364     0.364     0.364        11\n",
            "         108      0.824     0.875     0.848        16\n",
            "         109      0.615     0.800     0.696        10\n",
            "         110      0.833     0.625     0.714         8\n",
            "         111      0.643     0.818     0.720        11\n",
            "         112      0.533     0.667     0.593        12\n",
            "         113      0.727     0.533     0.615        15\n",
            "         114      0.385     0.385     0.385        13\n",
            "         116      0.846     0.917     0.880        12\n",
            "         117      0.929     0.929     0.929        14\n",
            "         118      0.700     0.778     0.737         9\n",
            "         120      0.562     0.900     0.692        10\n",
            "         122      0.800     0.500     0.615        16\n",
            "         125      0.818     0.562     0.667        16\n",
            "         126      0.846     0.917     0.880        12\n",
            "         127      0.692     0.900     0.783        10\n",
            "         129      0.556     0.455     0.500        11\n",
            "         130      0.357     0.500     0.417        10\n",
            "         131      1.000     0.800     0.889        10\n",
            "         132      0.929     0.929     0.929        14\n",
            "         133      0.500     0.600     0.545        10\n",
            "         134      0.467     0.583     0.519        12\n",
            "         135      0.923     0.923     0.923        13\n",
            "         136      0.615     0.889     0.727         9\n",
            "         137      0.556     0.667     0.606        15\n",
            "         139      0.889     0.889     0.889         9\n",
            "         142      1.000     1.000     1.000        12\n",
            "         143      0.625     0.455     0.526        11\n",
            "         144      0.857     0.857     0.857         7\n",
            "         145      0.533     0.667     0.593        12\n",
            "         146      0.333     0.571     0.421         7\n",
            "         147      0.522     0.800     0.632        15\n",
            "         148      0.929     0.812     0.867        16\n",
            "         150      0.786     0.688     0.733        16\n",
            "         152      0.800     0.727     0.762        11\n",
            "         153      0.778     0.875     0.824         8\n",
            "         154      0.429     0.600     0.500        10\n",
            "         155      0.769     0.833     0.800        12\n",
            "         157      0.400     0.333     0.364        12\n",
            "         160      1.000     1.000     1.000        17\n",
            "         161      0.917     0.846     0.880        13\n",
            "         162      0.909     0.909     0.909        11\n",
            "         163      0.417     0.556     0.476         9\n",
            "         164      0.611     0.917     0.733        12\n",
            "         167      0.824     0.824     0.824        17\n",
            "         168      0.857     1.000     0.923         6\n",
            "         169      0.231     0.231     0.231        13\n",
            "         171      0.786     0.917     0.846        12\n",
            "         172      0.909     1.000     0.952        10\n",
            "         174      0.750     0.750     0.750        12\n",
            "         176      0.875     0.636     0.737        11\n",
            "         177      0.857     0.857     0.857         7\n",
            "         179      0.933     1.000     0.966        14\n",
            "         180      0.533     0.471     0.500        17\n",
            "         182      0.625     0.625     0.625         8\n",
            "         183      0.833     0.714     0.769        14\n",
            "         185      0.533     0.615     0.571        13\n",
            "         187      0.467     0.700     0.560        10\n",
            "         189      0.917     0.611     0.733        18\n",
            "         193      0.667     0.600     0.632        10\n",
            "         195      0.583     0.875     0.700         8\n",
            "         196      0.778     0.875     0.824         8\n",
            "         197      0.929     0.867     0.897        15\n",
            "         198      0.857     0.923     0.889        13\n",
            "         199      0.706     0.857     0.774        14\n",
            "\n",
            "    accuracy                          0.728      1764\n",
            "   macro avg      0.734     0.734     0.725      1764\n",
            "weighted avg      0.740     0.728     0.725      1764\n",
            "\n",
            "[[12  0  0 ...  0  0  0]\n",
            " [ 1 15  0 ...  0  0  0]\n",
            " [ 0  0  6 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ... 13  0  0]\n",
            " [ 0  0  0 ...  0 12  0]\n",
            " [ 0  0  0 ...  0  0 12]]\n"
          ]
        }
      ],
      "source": [
        "# find the nearest neighbor based on the predicted attributes found in the image to infer the class\n",
        "knn = KNN(n_neighbors=1)\n",
        "msk = [x not in y_test_unseen for x in knn_y]\n",
        "knn.fit(knn_att[msk], knn_y[msk])\n",
        "# seen test classes\n",
        "test_preds = knn.predict(test_s_att_pred)\n",
        "print(classification_report(test_s['y'], test_preds, digits=3))\n",
        "print(confusion_matrix(test_s['y'], test_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjYnc6iQ8jtv"
      },
      "source": [
        "## ESZSL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIilztoTbQ0H"
      },
      "source": [
        "#### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HLiEGFvaHTus"
      },
      "outputs": [],
      "source": [
        "# get all the splits\n",
        "train, val, test_u, trainval_s, test_s, knn_att, knn_y, y_test_unseen, class_names = load_data('CUB', False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "IK86fX10HXli"
      },
      "outputs": [],
      "source": [
        "# put the data to correct shapes expected by the model\n",
        "labels_train = train['y'].reshape(-1, 1)\n",
        "labels_val = val['y'].reshape(-1, 1)\n",
        "train_labels_seen = np.unique(labels_train)\n",
        "val_labels_unseen = np.unique(labels_val)\n",
        "\n",
        "labels_trainval = trainval_s['y'].reshape(-1, 1)\n",
        "labels_test = test_u['y'].reshape(-1, 1)\n",
        "trainval_labels_seen = np.unique(labels_trainval)\n",
        "test_labels_unseen = np.unique(labels_test)\n",
        "\n",
        "labels_test_s = test_s['y'].reshape(-1, 1)\n",
        "test_labels_seen = np.unique(labels_test_s)\n",
        "\n",
        "train_vec = train['X'].T\n",
        "train_sig = knn_att[train_labels_seen].T\n",
        "\n",
        "val_vec = val['X'].T\n",
        "val_sig = knn_att[val_labels_unseen].T\n",
        "\n",
        "trainval_vec = trainval_s['X'].T\n",
        "trainval_sig = knn_att[trainval_labels_seen].T\n",
        "\n",
        "test_vec = test_u['X'].T\n",
        "test_sig = knn_att[test_labels_unseen].T\n",
        "\n",
        "test_s_vec = test_s['X'].T\n",
        "test_s_sig = knn_att[test_labels_seen].T\n",
        "\n",
        "lab_dict = {key: val for key, val in zip(train_labels_seen, np.arange(len(train_labels_seen)))}\n",
        "labels_train = np.array([lab_dict[lab[0]] for lab in labels_train]).reshape(-1,1)\n",
        "\n",
        "lab_dict = {key: val for key, val in zip(val_labels_unseen, np.arange(len(val_labels_unseen)))}\n",
        "labels_val = np.array([lab_dict[lab[0]] for lab in labels_val]).reshape(-1,1)\n",
        "\n",
        "lab_dict = {key: val for key, val in zip(trainval_labels_seen, np.arange(len(trainval_labels_seen)))}\n",
        "labels_trainval = np.array([lab_dict[lab[0]] for lab in labels_trainval]).reshape(-1,1)\n",
        "\n",
        "lab_dict = {key: val for key, val in zip(test_labels_unseen, np.arange(len(test_labels_unseen)))}\n",
        "labels_test = np.array([lab_dict[lab[0]] for lab in labels_test]).reshape(-1,1)\n",
        "\n",
        "lab_dict = {key: val for key, val in zip(test_labels_seen, np.arange(len(test_labels_seen)))}\n",
        "labels_test_s = np.array([lab_dict[lab[0]] for lab in labels_test_s]).reshape(-1,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "BWFdx5MNTCev"
      },
      "outputs": [],
      "source": [
        "# get the attributes of individual classes\n",
        "train_sig = knn_att[train_labels_seen].T\n",
        "trainval_sig = knn_att[trainval_labels_seen].T\n",
        "test_sig = knn_att[test_labels_unseen].T\n",
        "test_s_sig = knn_att[test_labels_seen].T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miQGhFesjnlJ"
      },
      "source": [
        "#### Fitting the ESZSL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "KjpEBVvO9-zL"
      },
      "outputs": [],
      "source": [
        "# prepare all the needed matrices\n",
        "\n",
        "#params for train and val set\n",
        "m_train = labels_train.shape[0]\n",
        "n_val = labels_val.shape[0]\n",
        "z_train = len(train_labels_seen)\n",
        "z1_val = len(val_labels_unseen)\n",
        "\n",
        "#params for trainval and test set\n",
        "m_trainval = labels_trainval.shape[0]\n",
        "n_test = labels_test.shape[0]\n",
        "z_trainval = len(trainval_labels_seen)\n",
        "z1_test = len(test_labels_unseen)\n",
        "\n",
        "#ground truth for train and val set\n",
        "gt_train = 0*np.ones((m_train, z_train))\n",
        "gt_train[np.arange(m_train), np.squeeze(labels_train)] = 1\n",
        "\n",
        "#grountruth for trainval and test set\n",
        "gt_trainval = 0*np.ones((m_trainval, z_trainval))\n",
        "gt_trainval[np.arange(m_trainval), np.squeeze(labels_trainval)] = 1\n",
        "\n",
        "#train set\n",
        "d_train = train_vec.shape[0]\n",
        "a_train = train_sig.shape[0]\n",
        "\n",
        "#Weights\n",
        "V = np.zeros((d_train,a_train))\n",
        "\n",
        "#trainval set\n",
        "d_trainval = trainval_vec.shape[0]\n",
        "a_trainval = trainval_sig.shape[0]\n",
        "W = np.zeros((d_trainval,a_trainval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFNIJKucj8Dh"
      },
      "outputs": [],
      "source": [
        "# finding the optimal hyperparameters\n",
        "accuracies = []\n",
        "als = []\n",
        "gas = []\n",
        "alphas = [-3,-2,-1,0,1,2,3]\n",
        "gammas = [-3,-2,-1,0,1,2,3]\n",
        "for alpha in alphas:\n",
        "    for gamma in gammas:\n",
        "        # find the solution\n",
        "        part_1 = np.linalg.pinv(np.matmul(train_vec, train_vec.transpose()) + (10**alpha)*np.eye(d_train))\n",
        "        part_0 = np.matmul(np.matmul(train_vec,gt_train),train_sig.transpose())\n",
        "        part_2 = np.linalg.pinv(np.matmul(train_sig, train_sig.transpose()) + (10**gamma)*np.eye(a_train))\n",
        "\n",
        "        V = np.matmul(np.matmul(part_1,part_0),part_2)\n",
        "\n",
        "        #predictions\n",
        "        outputs = np.matmul(np.matmul(val_vec.transpose(),V),val_sig)\n",
        "        preds = np.array([np.argmax(output) for output in outputs])\n",
        "\n",
        "        # save the performance and hyperparams\n",
        "        cm = confusion_matrix(labels_val, preds)\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        acc = sum(cm.diagonal())/len(val_labels_unseen)\n",
        "        accuracies.append(acc)\n",
        "        als.append(alpha)\n",
        "        gas.append(gamma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQSYPk3mkhr1"
      },
      "outputs": [],
      "source": [
        "# save the best hyperparameters\n",
        "df = pd.DataFrame({'alpha': als, 'gamma': gas, 'accuracy': accuracies})\n",
        "df.to_csv(os.path.join(PATH_ASSETS, 'eszsl_awa2_hyper.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "3AToBtn4j-CR"
      },
      "outputs": [],
      "source": [
        "# load the best hyperparameters\n",
        "df = pd.read_csv(os.path.join(PATH_ASSETS, 'eszsl_cub_hyper.csv'))\n",
        "best_params = df.iloc[np.argmax(df.accuracy)]\n",
        "alph1 = best_params['alpha']\n",
        "gamm1 = best_params['gamma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "L0hbFVPT-W1m"
      },
      "outputs": [],
      "source": [
        "# calculate the optimal weight matrix W\n",
        "part_1_test = np.linalg.pinv(np.matmul(trainval_vec, trainval_vec.transpose()) + (10**alph1)*np.eye(d_trainval))\n",
        "part_0_test = np.matmul(np.matmul(trainval_vec,gt_trainval),trainval_sig.transpose())\n",
        "part_2_test = np.linalg.pinv(np.matmul(trainval_sig, trainval_sig.transpose()) + (10**gamm1)*np.eye(a_trainval))\n",
        "\n",
        "W = np.matmul(np.matmul(part_1_test,part_0_test),part_2_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix with precision on the diagonal\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# prepare the labels\n",
        "labs = np.array(get_list_of_classes(class_names, 'CUB'))[test_labels_unseen]\n",
        "\n",
        "# predict values\n",
        "outputs_1 = np.matmul(np.matmul(test_vec.transpose(),W),test_sig)\n",
        "preds_1 = np.array([np.argmax(output) for output in outputs_1])\n",
        "\n",
        "# compute the probabilities\n",
        "cf_abs = confusion_matrix(labels_test, preds_1)\n",
        "cf_per = cf_abs / np.sum(cf_abs, axis=0)\n",
        "mat = pd.DataFrame((cf_per).round(2), index=labs, columns=labs)\n",
        "\n",
        "# plot the heatmap\n",
        "colormap = sn.color_palette(\"light:b\", as_cmap=True)\n",
        "sn.heatmap(mat, annot=False, fmt='.0%', cmap=colormap,  xticklabels=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        },
        "id": "oYVB80q73LPt",
        "outputId": "76a5b498-c9b1-4377-a490-211fb44c6da2"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ryaXJ9A-du8",
        "outputId": "0ef458ff-5003-4946-dab3-17638ef93a7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.39      0.47        56\n",
            "           1       0.51      0.70      0.59        60\n",
            "           2       0.09      0.02      0.03        59\n",
            "           3       0.71      0.45      0.55        60\n",
            "           4       0.81      0.72      0.76        60\n",
            "           5       0.38      0.87      0.53        60\n",
            "           6       0.55      1.00      0.71        60\n",
            "           7       0.53      0.85      0.65        60\n",
            "           8       0.53      0.95      0.68        60\n",
            "           9       0.73      0.18      0.29        60\n",
            "          10       0.57      0.63      0.60        59\n",
            "          11       0.74      0.23      0.35        60\n",
            "          12       0.43      0.92      0.58        60\n",
            "          13       0.27      0.48      0.35        50\n",
            "          14       0.29      0.03      0.06        60\n",
            "          15       0.54      0.64      0.59        59\n",
            "          16       0.35      0.62      0.45        60\n",
            "          17       0.56      0.58      0.57        60\n",
            "          18       0.76      0.83      0.79        60\n",
            "          19       0.67      0.95      0.79        60\n",
            "          20       0.83      0.98      0.90        60\n",
            "          21       0.57      0.53      0.55        60\n",
            "          22       0.42      0.25      0.32        59\n",
            "          23       0.51      0.58      0.54        60\n",
            "          24       0.64      0.77      0.70        60\n",
            "          25       0.39      0.39      0.39        59\n",
            "          26       0.51      0.53      0.52        59\n",
            "          27       0.29      0.23      0.26        60\n",
            "          28       0.60      0.72      0.65        60\n",
            "          29       0.46      0.88      0.60        58\n",
            "          30       0.62      0.68      0.65        60\n",
            "          31       0.33      0.22      0.26        60\n",
            "          32       0.67      0.20      0.31        60\n",
            "          33       0.62      0.61      0.62        59\n",
            "          34       0.48      0.22      0.30        60\n",
            "          35       0.44      0.66      0.53        59\n",
            "          36       0.58      0.44      0.50        59\n",
            "          37       0.65      0.28      0.40        60\n",
            "          38       0.80      0.78      0.79        60\n",
            "          39       0.80      0.66      0.72        53\n",
            "          40       0.46      0.18      0.26        60\n",
            "          41       0.63      0.40      0.49        60\n",
            "          42       1.00      0.07      0.12        60\n",
            "          43       0.55      0.51      0.53        59\n",
            "          44       0.68      0.35      0.46        60\n",
            "          45       0.87      0.65      0.74        60\n",
            "          46       0.62      0.17      0.26        60\n",
            "          47       0.54      0.63      0.58        60\n",
            "          48       0.62      0.38      0.47        60\n",
            "          49       0.42      0.95      0.58        60\n",
            "\n",
            "    accuracy                           0.54      2967\n",
            "   macro avg       0.56      0.54      0.51      2967\n",
            "weighted avg       0.56      0.54      0.51      2967\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#predictions on unseen classes\n",
        "outputs_1 = np.matmul(np.matmul(test_vec.transpose(),W),test_sig)\n",
        "preds_1 = np.array([np.argmax(output) for output in outputs_1])\n",
        "print(classification_report(labels_test, preds_1))\n",
        "clas_rep = classification_report(labels_test, preds_1 , output_dict=True)\n",
        "pd.DataFrame(clas_rep).T.to_csv(os.path.join(PATH_ASSETS, 'class_rep_unseen_awa2.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8-if2F-PDDI",
        "outputId": "9a60e3d7-5456-4041-c1e6-16e99af7e0f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.73      0.71        15\n",
            "           1       0.68      0.85      0.76        20\n",
            "           2       0.88      1.00      0.93         7\n",
            "           3       0.92      0.92      0.92        13\n",
            "           4       0.80      0.57      0.67        14\n",
            "           5       1.00      0.33      0.50        12\n",
            "           6       0.78      0.88      0.82         8\n",
            "           7       0.78      0.88      0.82         8\n",
            "           8       0.78      1.00      0.88         7\n",
            "           9       0.67      0.80      0.73         5\n",
            "          10       0.93      1.00      0.96        13\n",
            "          11       0.40      0.73      0.52        11\n",
            "          12       0.88      0.50      0.64        14\n",
            "          13       0.82      0.93      0.87        15\n",
            "          14       0.41      1.00      0.58        14\n",
            "          15       0.62      0.71      0.67         7\n",
            "          16       0.60      0.38      0.46         8\n",
            "          17       0.50      0.10      0.17        10\n",
            "          18       0.44      0.33      0.38        12\n",
            "          19       0.62      0.71      0.67         7\n",
            "          20       1.00      0.07      0.12        15\n",
            "          21       0.78      0.50      0.61        14\n",
            "          22       0.73      0.85      0.79        13\n",
            "          23       0.75      0.82      0.78        11\n",
            "          24       0.33      0.67      0.44         9\n",
            "          25       0.64      1.00      0.78         9\n",
            "          26       0.42      1.00      0.59        10\n",
            "          27       0.68      0.93      0.79        14\n",
            "          28       0.86      0.43      0.57        14\n",
            "          29       0.75      0.50      0.60        12\n",
            "          30       0.77      0.77      0.77        13\n",
            "          31       0.86      1.00      0.92        12\n",
            "          32       0.80      0.89      0.84         9\n",
            "          33       0.33      0.08      0.13        12\n",
            "          34       0.20      0.09      0.13        11\n",
            "          35       0.78      0.93      0.85        15\n",
            "          36       0.00      0.00      0.00        12\n",
            "          37       0.58      1.00      0.73        11\n",
            "          38       0.60      0.43      0.50        14\n",
            "          39       0.00      0.00      0.00        12\n",
            "          40       0.64      0.69      0.67        13\n",
            "          41       0.31      0.62      0.42         8\n",
            "          42       0.57      0.67      0.62        12\n",
            "          43       0.67      0.18      0.29        11\n",
            "          44       0.82      1.00      0.90         9\n",
            "          45       0.56      0.71      0.63         7\n",
            "          46       0.56      1.00      0.71        10\n",
            "          47       0.54      1.00      0.70        15\n",
            "          48       0.67      0.50      0.57        12\n",
            "          49       0.80      0.33      0.47        12\n",
            "          50       0.88      0.93      0.90        15\n",
            "          51       0.28      1.00      0.44         7\n",
            "          52       0.75      0.92      0.83        13\n",
            "          53       0.75      0.86      0.80        14\n",
            "          54       0.88      1.00      0.93         7\n",
            "          55       1.00      1.00      1.00        12\n",
            "          56       0.64      0.50      0.56        14\n",
            "          57       0.73      1.00      0.85        11\n",
            "          58       0.47      0.64      0.54        11\n",
            "          59       0.68      1.00      0.81        13\n",
            "          60       0.00      0.00      0.00        14\n",
            "          61       0.86      0.92      0.89        13\n",
            "          62       0.31      0.71      0.43         7\n",
            "          63       0.69      0.56      0.62        16\n",
            "          64       0.67      0.73      0.70        11\n",
            "          65       0.55      1.00      0.71         6\n",
            "          66       0.80      0.31      0.44        13\n",
            "          67       0.50      0.33      0.40         9\n",
            "          68       0.44      0.33      0.38        12\n",
            "          69       1.00      0.27      0.43        11\n",
            "          70       0.67      0.50      0.57        12\n",
            "          71       0.92      0.65      0.76        17\n",
            "          72       0.21      0.60      0.32         5\n",
            "          73       0.90      0.56      0.69        16\n",
            "          74       0.62      0.56      0.59         9\n",
            "          75       1.00      0.27      0.42        15\n",
            "          76       1.00      0.29      0.44        14\n",
            "          77       0.76      0.87      0.81        15\n",
            "          78       0.72      0.87      0.79        15\n",
            "          79       0.60      0.50      0.55        12\n",
            "          80       0.77      0.91      0.83        11\n",
            "          81       0.64      0.82      0.72        17\n",
            "          82       0.71      0.38      0.50        13\n",
            "          83       0.44      0.29      0.35        14\n",
            "          84       0.32      0.60      0.41        10\n",
            "          85       0.30      0.27      0.29        11\n",
            "          86       0.82      0.88      0.85        16\n",
            "          87       1.00      0.50      0.67        10\n",
            "          88       1.00      0.38      0.55         8\n",
            "          89       0.86      0.55      0.67        11\n",
            "          90       0.67      0.33      0.44        12\n",
            "          91       0.60      0.60      0.60        15\n",
            "          92       0.67      0.31      0.42        13\n",
            "          93       1.00      0.83      0.91        12\n",
            "          94       0.72      0.93      0.81        14\n",
            "          95       0.57      0.44      0.50         9\n",
            "          96       0.70      0.70      0.70        10\n",
            "          97       0.70      0.44      0.54        16\n",
            "          98       0.75      0.56      0.64        16\n",
            "          99       0.89      0.67      0.76        12\n",
            "         100       0.45      0.50      0.48        10\n",
            "         101       1.00      0.09      0.17        11\n",
            "         102       0.86      0.60      0.71        10\n",
            "         103       0.50      0.60      0.55        10\n",
            "         104       0.92      0.86      0.89        14\n",
            "         105       0.71      0.50      0.59        10\n",
            "         106       0.60      0.25      0.35        12\n",
            "         107       0.92      0.92      0.92        13\n",
            "         108       0.35      1.00      0.51         9\n",
            "         109       0.67      0.67      0.67        15\n",
            "         110       0.69      1.00      0.82         9\n",
            "         111       1.00      1.00      1.00        12\n",
            "         112       0.55      0.55      0.55        11\n",
            "         113       0.86      0.86      0.86         7\n",
            "         114       0.46      0.92      0.61        12\n",
            "         115       0.25      0.14      0.18         7\n",
            "         116       0.64      0.60      0.62        15\n",
            "         117       0.78      0.88      0.82        16\n",
            "         118       0.79      0.69      0.73        16\n",
            "         119       0.67      0.73      0.70        11\n",
            "         120       0.78      0.88      0.82         8\n",
            "         121       0.43      0.30      0.35        10\n",
            "         122       0.46      0.92      0.61        12\n",
            "         123       0.20      0.08      0.12        12\n",
            "         124       0.86      0.71      0.77        17\n",
            "         125       0.81      1.00      0.90        13\n",
            "         126       0.67      0.91      0.77        11\n",
            "         127       0.20      0.56      0.29         9\n",
            "         128       0.70      0.58      0.64        12\n",
            "         129       0.75      0.88      0.81        17\n",
            "         130       0.50      1.00      0.67         6\n",
            "         131       0.25      0.62      0.36        13\n",
            "         132       0.80      0.33      0.47        12\n",
            "         133       0.67      1.00      0.80        10\n",
            "         134       0.80      0.67      0.73        12\n",
            "         135       0.56      0.82      0.67        11\n",
            "         136       0.83      0.71      0.77         7\n",
            "         137       0.92      0.79      0.85        14\n",
            "         138       0.00      0.00      0.00        17\n",
            "         139       0.42      0.62      0.50         8\n",
            "         140       0.89      0.57      0.70        14\n",
            "         141       0.54      0.54      0.54        13\n",
            "         142       0.57      0.40      0.47        10\n",
            "         143       0.77      0.56      0.65        18\n",
            "         144       0.78      0.70      0.74        10\n",
            "         145       0.40      0.75      0.52         8\n",
            "         146       0.67      0.75      0.71         8\n",
            "         147       0.64      0.93      0.76        15\n",
            "         148       0.91      0.77      0.83        13\n",
            "         149       1.00      0.43      0.60        14\n",
            "\n",
            "    accuracy                           0.64      1764\n",
            "   macro avg       0.66      0.65      0.62      1764\n",
            "weighted avg       0.67      0.64      0.62      1764\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#predictions on seen test classes\n",
        "outputs_1 = np.matmul(np.matmul(test_s_vec.transpose(),W),test_s_sig)\n",
        "preds_1 = np.array([np.argmax(output) for output in outputs_1])\n",
        "print(classification_report(labels_test_s, preds_1))\n",
        "clas_rep = classification_report(labels_test_s, preds_1 , output_dict=True)\n",
        "pd.DataFrame(clas_rep).T.to_csv(os.path.join(PATH_ASSETS, 'class_rep_seen_awa2.csv'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWpFstrVR3ag"
      },
      "source": [
        "#### Enhance the attributes with bert embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f5fMQKh20FGQ"
      },
      "outputs": [],
      "source": [
        "# extract the name of the class\n",
        "def get_list_of_classes(class_names, dataset='CUB'):\n",
        "    if dataset=='CUB':\n",
        "        class_list = sorted([(int(cls[0][0].split('.')[0])-1, cls[0][0].split('.')[1].replace('_', ' ')) for cls in class_names])\n",
        "        class_list = [tup[1] for tup in class_list]\n",
        "    elif dataset=='AWA2':\n",
        "        class_list = [element[0][0].replace('+', ' ') for element in class_names]\n",
        "    return class_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpXLQfWoSNHa",
        "outputId": "1f50193a-288e-490e-b185-322b127d7369"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# Load BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f_L9TKlR6Yb"
      },
      "outputs": [],
      "source": [
        "# get clean names of the classes\n",
        "class_list = get_list_of_classes(class_names, dataset='AWA2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ba4cdce77ca746f4bbf9b21ab9a3f910",
            "d6a5f6faf918468799e8dbcabfa4d27c",
            "3abeadb6f47640c99cc92ca05f86a92a",
            "9186d08f86584ddcb3c9ae7af6f0fd59",
            "daf3ba88417d4db48ad1f3e4c03644a3",
            "6a09ddd29ada4b4f87800e41073c0b6e",
            "38b8bb82d10e4ebf8ec2e08c999dc0f4",
            "cf5054f325f64def9ab4f477aa2a9434",
            "2640957a3e294810a7717324edddfa0f",
            "4a2391265adc46b5bbdd8c03028c1c2a",
            "b60a1885c05b47d8998edd11a4ea96ae"
          ]
        },
        "id": "ygG5rip9R9wX",
        "outputId": "0932a8af-b655-4331-a30e-0ced42015bec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba4cdce77ca746f4bbf9b21ab9a3f910",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# get the embeddings corresponding the the label\n",
        "class_dict = dict()\n",
        "for name in tqdm(class_list):\n",
        "    # Tokenize the word\n",
        "    tokens = tokenizer.tokenize(name)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # Generate word embeddings\n",
        "    input_ids = torch.tensor([input_ids])\n",
        "    outputs = model(input_ids)\n",
        "\n",
        "    # Retrieve word embedding for the [CLS] token\n",
        "    embedding = outputs.last_hidden_state[:, 0, :]\n",
        "    class_dict[name] = embedding\n",
        "bert_attributes = np.array([emb.detach().numpy()[0] for emb in class_dict.values()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIdD9uMrSRq-",
        "outputId": "66dc395f-81b0-4c0d-bf6f-33214947694f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preserved ratio of variance after applying PCA:  0.5766022652387619\n"
          ]
        }
      ],
      "source": [
        "# reduce the dimensionality of the bert embeddings\n",
        "pca = PCA(10)\n",
        "bert_pca = pca.fit_transform(bert_attributes)\n",
        "print('Preserved ratio of variance after applying PCA: ', sum(pca.explained_variance_ratio_))\n",
        "bert_pca = (bert_pca - bert_pca.min(axis=0)) / (bert_pca.max(axis=0) - bert_pca.min(axis=0))\n",
        "# concatenate bert embeddings to the original attributes\n",
        "knn_att_plus = np.concatenate((knn_att, bert_pca), axis=1)\n",
        "train_sig = knn_att_plus[train_labels_seen].T\n",
        "trainval_sig = knn_att_plus[trainval_labels_seen].T\n",
        "test_sig = knn_att_plus[test_labels_unseen].T\n",
        "test_s_sig = knn_att_plus[test_labels_seen].T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVLR5NM4eabU"
      },
      "outputs": [],
      "source": [
        "# prepare all the needed matrices\n",
        "\n",
        "#params for train and val set\n",
        "m_train = labels_train.shape[0]\n",
        "n_val = labels_val.shape[0]\n",
        "z_train = len(train_labels_seen)\n",
        "z1_val = len(val_labels_unseen)\n",
        "\n",
        "#params for trainval and test set\n",
        "m_trainval = labels_trainval.shape[0]\n",
        "n_test = labels_test.shape[0]\n",
        "z_trainval = len(trainval_labels_seen)\n",
        "z1_test = len(test_labels_unseen)\n",
        "\n",
        "#ground truth for train and val set\n",
        "gt_train = 0*np.ones((m_train, z_train))\n",
        "gt_train[np.arange(m_train), np.squeeze(labels_train)] = 1\n",
        "\n",
        "#grountruth for trainval and test set\n",
        "gt_trainval = 0*np.ones((m_trainval, z_trainval))\n",
        "gt_trainval[np.arange(m_trainval), np.squeeze(labels_trainval)] = 1\n",
        "\n",
        "#train set\n",
        "d_train = train_vec.shape[0]\n",
        "a_train = train_sig.shape[0]\n",
        "\n",
        "#Weights\n",
        "V = np.zeros((d_train,a_train))\n",
        "\n",
        "#trainval set\n",
        "d_trainval = trainval_vec.shape[0]\n",
        "a_trainval = trainval_sig.shape[0]\n",
        "W = np.zeros((d_trainval,a_trainval))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2eZMYfRej64"
      },
      "outputs": [],
      "source": [
        "# calculate the optimal weight matrix W\n",
        "part_1_test = np.linalg.pinv(np.matmul(trainval_vec, trainval_vec.transpose()) + (10**alph1)*np.eye(d_trainval))\n",
        "part_0_test = np.matmul(np.matmul(trainval_vec,gt_trainval),trainval_sig.transpose())\n",
        "part_2_test = np.linalg.pinv(np.matmul(trainval_sig, trainval_sig.transpose()) + (10**gamm1)*np.eye(a_trainval))\n",
        "\n",
        "W = np.matmul(np.matmul(part_1_test,part_0_test),part_2_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlsYIt0_emur",
        "outputId": "13facb87-f2d3-46e2-abb3-9ce22c403b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.48      0.35      0.40      1645\n",
            "           1       0.10      1.00      0.18       174\n",
            "           2       0.45      0.52      0.48      1420\n",
            "           3       0.26      0.13      0.17       988\n",
            "           4       0.41      0.21      0.27       383\n",
            "           5       0.75      0.51      0.61      1202\n",
            "           6       0.56      0.50      0.53       310\n",
            "           7       0.88      0.35      0.50       630\n",
            "           8       0.09      0.13      0.10       215\n",
            "           9       0.04      0.04      0.04       946\n",
            "\n",
            "    accuracy                           0.35      7913\n",
            "   macro avg       0.40      0.37      0.33      7913\n",
            "weighted avg       0.45      0.35      0.37      7913\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#predictions on unseen classes\n",
        "outputs_1 = np.matmul(np.matmul(test_vec.transpose(),W),test_sig)\n",
        "preds_1 = np.array([np.argmax(output) for output in outputs_1])\n",
        "print(classification_report(labels_test, preds_1))\n",
        "clas_rep = classification_report(labels_test, preds_1 , output_dict=True)\n",
        "pd.DataFrame(clas_rep).T.to_csv(os.path.join(PATH_ASSETS, 'class_rep_unseen_bert_awa2.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1w3jYHBeyGc",
        "outputId": "96e07d81-1d20-4108-d213-0235315ac5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.95      0.90       182\n",
            "           1       0.96      0.93      0.95       162\n",
            "           2       1.00      0.85      0.92        60\n",
            "           3       0.78      0.61      0.68        41\n",
            "           4       1.00      0.96      0.98       107\n",
            "           5       0.89      0.92      0.90       148\n",
            "           6       0.95      0.79      0.86       218\n",
            "           7       0.90      0.86      0.88       100\n",
            "           8       1.00      0.94      0.97        31\n",
            "           9       0.92      0.92      0.92        25\n",
            "          10       1.00      1.00      1.00       175\n",
            "          11       0.94      0.91      0.93       141\n",
            "          12       1.00      1.00      1.00       152\n",
            "          13       0.80      0.88      0.84       150\n",
            "          14       0.88      0.54      0.67        54\n",
            "          15       0.89      1.00      0.94       144\n",
            "          16       0.96      0.99      0.98       189\n",
            "          17       0.89      0.99      0.94       163\n",
            "          18       0.62      0.94      0.74       140\n",
            "          19       0.94      0.97      0.96       124\n",
            "          20       0.88      0.88      0.88       162\n",
            "          21       0.91      0.88      0.90       139\n",
            "          22       0.92      1.00      0.96       239\n",
            "          23       0.93      0.90      0.91       135\n",
            "          24       0.96      1.00      0.98       211\n",
            "          25       0.97      0.84      0.90       119\n",
            "          26       0.69      0.94      0.80       119\n",
            "          27       0.85      0.68      0.76        50\n",
            "          28       0.88      0.92      0.90       166\n",
            "          29       0.93      0.92      0.92       197\n",
            "          30       1.00      1.00      1.00       228\n",
            "          31       0.98      0.99      0.99       182\n",
            "          32       0.87      0.89      0.88       275\n",
            "          33       0.98      0.84      0.91       155\n",
            "          34       0.98      0.98      0.98       198\n",
            "          35       0.80      0.20      0.32        40\n",
            "          36       0.97      0.99      0.98       184\n",
            "          37       0.94      0.90      0.92       219\n",
            "          38       0.84      0.93      0.88       104\n",
            "          39       0.94      0.69      0.80       254\n",
            "\n",
            "    accuracy                           0.91      5882\n",
            "   macro avg       0.91      0.88      0.89      5882\n",
            "weighted avg       0.92      0.91      0.91      5882\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#predictions on seen test classes\n",
        "outputs_1 = np.matmul(np.matmul(test_s_vec.transpose(),W),test_s_sig)\n",
        "preds_1 = np.array([np.argmax(output) for output in outputs_1])\n",
        "print(classification_report(labels_test_s, preds_1))\n",
        "clas_rep = classification_report(labels_test_s, preds_1 , output_dict=True)\n",
        "pd.DataFrame(clas_rep).T.to_csv(os.path.join(PATH_ASSETS, 'class_rep_seen_bert_awa2.csv'))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2640957a3e294810a7717324edddfa0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38b8bb82d10e4ebf8ec2e08c999dc0f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3abeadb6f47640c99cc92ca05f86a92a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf5054f325f64def9ab4f477aa2a9434",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2640957a3e294810a7717324edddfa0f",
            "value": 50
          }
        },
        "4a2391265adc46b5bbdd8c03028c1c2a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a09ddd29ada4b4f87800e41073c0b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9186d08f86584ddcb3c9ae7af6f0fd59": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a2391265adc46b5bbdd8c03028c1c2a",
            "placeholder": "",
            "style": "IPY_MODEL_b60a1885c05b47d8998edd11a4ea96ae",
            "value": " 50/50 [00:04&lt;00:00, 10.39it/s]"
          }
        },
        "b60a1885c05b47d8998edd11a4ea96ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba4cdce77ca746f4bbf9b21ab9a3f910": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6a5f6faf918468799e8dbcabfa4d27c",
              "IPY_MODEL_3abeadb6f47640c99cc92ca05f86a92a",
              "IPY_MODEL_9186d08f86584ddcb3c9ae7af6f0fd59"
            ],
            "layout": "IPY_MODEL_daf3ba88417d4db48ad1f3e4c03644a3"
          }
        },
        "cf5054f325f64def9ab4f477aa2a9434": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6a5f6faf918468799e8dbcabfa4d27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a09ddd29ada4b4f87800e41073c0b6e",
            "placeholder": "",
            "style": "IPY_MODEL_38b8bb82d10e4ebf8ec2e08c999dc0f4",
            "value": "100%"
          }
        },
        "daf3ba88417d4db48ad1f3e4c03644a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}